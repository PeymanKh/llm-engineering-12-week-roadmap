{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927ad0c8-64d0-4a9c-ae45-35d1d86c9d69",
   "metadata": {},
   "source": [
    "# Research Assistant Agent\n",
    "\n",
    "The goal of this notebook is to prototype a multi-agent system that strategically gathers and uses context in order to generate more reliable answers. Inspired by the [STORM](https://arxiv.org/pdf/2402.14207) research paper by Stanford University, this is a custom implementation of `Multi-Perspective Question Answering` where the system looks at the question from different perspectives and starts multiple conversations with experts for the research phase. To understand the concept in more detail, please follow along in this notebook.\n",
    "\n",
    "The system has the following features:\n",
    "1. **Planning:** Users provide a topic, and the system generates a team of AI analysts, each focusing on one sub-topic. `Human-in-the-loop` will be used to refine the sub-topics before research begins.\n",
    "2. **LLM Utilization:** Each analyst will conduct in-depth interviews with an expert agent with web search capabilities. The interview will be a multi-turn conversation to extract detailed insights.\n",
    "3. **Research Process:** Expert agents will gather information to answer analyst questions in `parallel` using `Map-Reduce` technique for efficiency.\n",
    "4. **Output Format:** The gathered insights from each interview will be synthesized into a flexible output format using customizable prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e1d3ee-3562-4b11-924b-c6140c0cc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import re\n",
    "import sys, os\n",
    "import logging\n",
    "import operator\n",
    "from typing import Any, List, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "from langgraph.types import Send\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab861401-85d1-40d1-9726-765295bc45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all debug logs from urllib3 and langsmith\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"langsmith\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"openai._base_client\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "# Optional: also raise the global root logger level\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b225b01-3ea0-4175-8208-cdb0ed65249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 17:19:45,525 - root - INFO - Configuration loaded for environment: development\n"
     ]
    }
   ],
   "source": [
    "# Import settings and sensitive variables from config\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "from codes.config.config import config\n",
    "\n",
    "# Set langsmith variables for tracing\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = config.langsmith_api_key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"storm-research-assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a2f7e32-fa03-4d4a-b222-c94dc1b2ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ChatModel instance\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=config.openai_api_key.get_secret_value()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d93b68-63bf-4c53-8887-7ed04d29c573",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Analysts\n",
    "As mentioned in the [research paper](https://arxiv.org/pdf/2402.14207), each topic or question has varying aspects and when individuals with different perspectives want to research about it, they concentrate on different aspects. That is why first agent of the system takes the problem as input and define perspectives (Which we call analyst in this project). \n",
    "\n",
    "In order to have a more reliable wokflow, Human-in-the-loop has been implemented to approve the perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a6e41e-c9da-4a57-9be4-27a906bff948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State and schemas for the first agent\n",
    "class Analyst(BaseModel):\n",
    "    \"\"\"\n",
    "    Schema for a single analyst.\n",
    "    Note that we generate both second person and third person descriptions for the perspective. \n",
    "    The reason is to enhance prompt engineering of upcomming agents.\n",
    "        \n",
    "    - description_2nd example: You are ...\n",
    "    - description_3rd example: He/She is ...\n",
    "    \"\"\"\n",
    "    affiliation: str = Field(\n",
    "        description=\"primary affiliation of the analyst.\"\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"name of the analyst.\"\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"role of the analyst in the context of topic.\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"third-person description of the analyst focus, concerns, and motivations.\"\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return (\n",
    "            f\"Name: {self.name}\\n\"\n",
    "            f\"Role: {self.role}\\n\"\n",
    "            f\"Affiliation: {self.affiliation}\\n\"\n",
    "            f\"description: {self.description}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "class CreateAnalystOutput(BaseModel):\n",
    "    \"\"\"\n",
    "    Schema for create_analyst node's output.\n",
    "    we use .with_structured_output to bind this output schema to ChatModel.\n",
    "    \"\"\"\n",
    "    analysts : list[Analyst] = Field(\n",
    "        description=\"comprehensive list of analysts with their role and affiliation.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class CreateAnalystState(TypedDict):\n",
    "    \"\"\"State for the generate analyst graph. we pass this state and the graph edit the field on invokation\"\"\"\n",
    "    topic: str  # Research topic\n",
    "    max_analysts: int  # Number of analysts we want to have\n",
    "    human_feedback: str  # Human feedback for editing generated analystst (for Human-in-the-loop)\n",
    "    analysts: list[Analyst]  # List of generated analysts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538d5505-0673-4a23-a50b-b5ce55aebd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "generate_analyst_prompt_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a **persona designer**. \n",
    "Your goal is to receive a research topic or problem from the user and produce distinct analyst personas who will analyze the problem \n",
    "from multiple, non-overlapping points of view. The user provides you with topic, and optional human_feedback as input. \n",
    "\n",
    "Follow the following instructions carefully to generate the response:\n",
    "    \n",
    "1. First review the research topic or problem.\n",
    "\n",
    "2. Examine any editorial feedback that has been optionally provided to guide creation of the personas.\n",
    "\n",
    "3. Create analyst personas that differ in expertise, background, and motivationso that, together, they cover all key aspects needed to analyze the topic.\n",
    "\n",
    "4. Return the top {max_analysts} personas.\n",
    "\"\"\"),\n",
    "        \n",
    "        (\"user\", \"\"\"Research Topic: ```{topic}```\n",
    "Human Feedback (optional): ```{human_feedback}```\n",
    "\"\"\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9cc43cf-139c-4922-b4f5-e47f177d504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def create_analysts(state: CreateAnalystState):\n",
    "    \"\"\"Node to create list of different analysts(perspectives)\"\"\"\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "    max_analysts = state[\"max_analysts\"]\n",
    "    human_feedback = state.get(\"human_feedback\", \"\")\n",
    "\n",
    "    # Add output schema to the ChatModel\n",
    "    structured_llm = llm.with_structured_output(CreateAnalystOutput)\n",
    "\n",
    "    # Create chat messages from prompt templates\n",
    "    prompt = generate_analyst_prompt_template.invoke({\n",
    "        \"topic\": topic,\n",
    "        \"max_analysts\": max_analysts,\n",
    "        \"human_feedback\": human_feedback\n",
    "    })\n",
    "\n",
    "    # Create analysts with LLM invokation\n",
    "    analysts = structured_llm.invoke(prompt.messages)\n",
    "\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "\n",
    "def human_feedback(state: CreateAnalystState):\n",
    "    \"No-op node that should be interruped on for Human-in-the-loop feedback\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def human_feedback_router(state: CreateAnalystState):\n",
    "    \"\"\"\n",
    "    Returns the next node to execute. \n",
    "    If feedback is provided, the router points to regenerating analysts(personas), otherwise continutes.\n",
    "    \"\"\"\n",
    "    human_feedback = state.get(\"human_feedback\", None)\n",
    "\n",
    "    if human_feedback:\n",
    "        return \"create_analysts\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5035914a-4cf8-45bd-8edb-401e777eefa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph\n",
    "analyst_builder = StateGraph(CreateAnalystState)\n",
    "\n",
    "analyst_builder.add_node(\"create_analysts\", create_analysts)\n",
    "analyst_builder.add_node(\"human_feedback\", human_feedback)\n",
    "\n",
    "analyst_builder.add_edge(START, \"create_analysts\")\n",
    "analyst_builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "analyst_builder.add_conditional_edges(\"human_feedback\", human_feedback_router, [\"create_analysts\", END])\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "analyst_graph = analyst_builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "\n",
    "# View Graph\n",
    "display(Image(analyst_graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d48eb7d4-1dfa-4ca9-94f6-ed66e60e58a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Chen\n",
      "Affiliation: University of Computer Science\n",
      "Role: Computer Science Educator\n",
      "description: Dr. Emily Chen is a renowned computer science educator with a focus on programming languages. She emphasizes Python's role in AI education due to its simplicity and readability, which make it an ideal language for students and educators alike. Her perspective highlights Python's built-in libraries and how its syntax lowers barriers to entry for beginners, thereby fostering a larger community of aspiring AI developers.\n",
      "--------------------------------------------------\n",
      "Name: Raj Patel\n",
      "Affiliation: TechCorp Innovations\n",
      "Role: AI Systems Developer\n",
      "description: Raj Patel is an experienced AI systems developer working at TechCorp Innovations. He advocates for Python as the best language for AI development due to its extensive ecosystem of libraries and frameworks like TensorFlow and PyTorch, which facilitate rapid prototyping and deployment of AI solutions. His viewpoint emphasizes Python's flexibility and its active community that continually contributes to its growth and adaptability to new AI trends and challenges.\n",
      "--------------------------------------------------\n",
      "Name: Lina Rivera\n",
      "Affiliation: Global Analytics Group\n",
      "Role: Data Scientist\n",
      "description: Lina Rivera is a data scientist at Global Analytics Group who leverages Python for its robust data handling and visualization capabilities. She focuses on Python's ability to integrate seamlessly with data analysis tools, enabling efficient processing of large datasets, which is crucial in AI development. Lina advocates for Python's role in exploratory data analysis and its comprehensive library support, which aids in developing complex AI models to derive meaningful insights.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "max_analysts = 3\n",
    "topic = \"Why python is the best language for AI development?\"\n",
    "thread = {\"configurable\": {\"thread_id\": 1}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in analyst_graph.stream({\"topic\":topic,\"max_analysts\":max_analysts,}, thread, stream_mode=\"values\"):\n",
    "    # Review\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"description: {analyst.description}\")\n",
    "            print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0070a5e3-cf26-436d-9b87-eca31a5b57c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = analyst_graph.get_state(thread)\n",
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ac64526-ef0c-4641-aee1-d9f75bb95d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f093093-c346-6ae6-8002-8ea365a9af77'}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update state\n",
    "analyst_graph.update_state(thread, {\n",
    "    \"human_feedback\": \"include one persona from startup view, one data scientist, and one ai engineer\"\n",
    "},as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9991efc0-c5a9-44cd-8c96-c0b2cc3de1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Jane Smith\n",
      "Affiliation: University of Technology's Department of Computer Science\n",
      "Role: Academic Researcher in Programming Languages\n",
      "Description: Dr. Jane Smith is an academic researcher with a focus on programming languages and their applications in various domains, including artificial intelligence. She explores the technical strengths and weaknesses of different programming languages, with particular interest in syntax complexity, execution speed, and library support, to determine their suitability for AI projects. Her analysis includes comparison with other languages commonly used in AI development to contextualize Python's advantages and limitations.\n",
      "--------------------------------------------------\n",
      "Name: Mark Johnson\n",
      "Affiliation: Tech Insights Journal\n",
      "Role: Tech Industry Analyst\n",
      "Description: Mark Johnson is a tech industry analyst who examines trends and adoption rates of technologies within the industry. He focuses on why businesses and developers prefer certain programming languages, considering factors such as community support, integration with existing systems, and long-term strategic benefits. His motivation is to provide insights that help stakeholders make informed decisions about technology investments and to predict future trends in AI language adoption.\n",
      "--------------------------------------------------\n",
      "Name: Emily Tran\n",
      "Affiliation: Open Source Python Foundation\n",
      "Role: Open Source Advocate and Developer\n",
      "Description: Emily Tran is an open-source advocate and active developer in the Python community. She is dedicated to promoting Python's accessibility and versatility for new AI developers. Her focus is on the ease of use, availability of well-maintained libraries, and the strength of community resources that enhance Python's appeal for AI development. Emily aims to emphasize how Python's growth and evolution are driven by its open-source nature and collaborative ecosystem, making it an attractive choice for AI projects.\n",
      "--------------------------------------------------\n",
      "Name: Emily Clarke\n",
      "Affiliation: Tech Startup 'InnovAI'\n",
      "Role: Startup Entrepreneur\n",
      "Description: Emily Clarke is a startup entrepreneur who believes in leveraging cutting-edge technologies to propel her tech startup, InnovAI, to new heights. She is particularly focused on optimizing resource allocation and ensuring rapid prototyping. Emily advocates for Python in AI development due to its extensive ecosystem, community support, and versatility, which aligns perfectly with the fast-paced, iterative nature of startup environments. Her primary concerns include maximizing productivity, minimizing costs, and ensuring a smooth learning curve for new developers joining her team.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Alan Gupta\n",
      "Affiliation: Data Science Research Lab\n",
      "Role: Lead Data Scientist\n",
      "Description: Dr. Alan Gupta is a lead data scientist at a renowned data science research lab. He is highly invested in the analytical and algorithmic aspects of artificial intelligence. Dr. Gupta values Python for its rich library support and its ability to facilitate complex data manipulation and statistical modeling. His primary focus is on Python's capabilities to streamline data processing tasks and the role its vast collection of libraries, like Pandas and NumPy, play in advancing research efficiency and efficacy. He is motivated by the ease with which Python integrates with data visualization tools and its contribution to reproducible research.\n",
      "--------------------------------------------------\n",
      "Name: Sophia Li\n",
      "Affiliation: TechCorp AI Solutions\n",
      "Role: AI Engineer\n",
      "Description: Sophia Li is an astute AI engineer working for TechCorp AI Solutions, where she is responsible for developing and deploying AI models in production environments. She appreciates Python for its simplicity and readability, which facilitate collaboration within large engineering teams. Sophia is particularly concerned with Python's robustness and interoperability with other languages, which are crucial for scaling AI models and ensuring seamless integration with existing technology stacks. She is driven by the need for efficient model deployment and real-time application performance, where Python's frameworks like TensorFlow and PyTorch play a pivotal role.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for event in analyst_graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "188d207d-2a15-42e7-b909-a84b3818c7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state = analyst_graph.get_state(thread)\n",
    "final_state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e43ff07c-185d-420e-9973-c60cef15b120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f092f3b-2aec-6e7a-8004-9a2e548ed900'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we are satisfied, then we simply supply no feedback\n",
    "analyst_graph.update_state(thread, {\"human_feedback\": None},as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13377693-427f-4b7a-b0e8-8246fae5d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Jamie Patel\n",
      "Affiliation: Emerging AI Startup\n",
      "Role: Startup Founder\n",
      "Description: Jamie is the founder of a small but promising AI startup. Their focus is on leveraging innovative AI solutions to solve everyday problems. Jamie is particularly interested in Python because of its extensive support community and the ecosystem of libraries that allow rapid prototyping and deployment, which is crucial for startups to stay agile and responsive to market demands. They advocate for Python as the backbone of their development stack due to its versatility and ease of learning, which helps in efficiently scaling their team with new developers as the company grows.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Amelia Wang\n",
      "Affiliation: Leading Data Science Consultancy\n",
      "Role: Senior Data Scientist\n",
      "Description: Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\n",
      "--------------------------------------------------\n",
      "Name: Ravi Kumar\n",
      "Affiliation: Top Global Tech Company\n",
      "Role: AI Engineer\n",
      "Description: Ravi Kumar is an AI Engineer specializing in the development and deployment of neural networks. Working for a tech leader, Ravi appreciates Python primarily for TensorFlow and PyTorch, the powerful libraries it offers for AI development. He values Python's ability to manage complex algorithms while allowing iterative problem-solving approaches. Ravi's motivation lies in Python's cross-platform capabilities and the ease of integration with other technologies, enabling him to build robust AI models efficiently. His enthusiasm for Python extends to its role in accelerating development cycles, making it indispensable in the AI toolkit.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for event in analyst_graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf80144c-0707-4d77-9571-046d2f37dc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state = analyst_graph.get_state(thread)\n",
    "final_state.next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067a0c7-0165-4077-b725-20e6280b3346",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Research Phase\n",
    "After successfully implementing the Anlyst (perspective) generation with Human-in-the-loop for verification, next step is to implement research phase. In this part each analyst will go through a conversation with an expert capable of searching the web and Wikipedia.\n",
    "The reason for choosing conversation between analyst and expert is inspired by the idea of while we find answer to our question, simultaneously we face new question. that is why conversation with multiple turns is choosed for this part to gather in-depth context about the topic/question asked by user.\n",
    "\n",
    "Research phase has following flow:\n",
    "1. Generate question to be asked from the expert.\n",
    "2. Generate search query to find the answer of question from the web.\n",
    "3. search the Web and Wikipedia in parallel.\n",
    "4. generate answer to question by gathered context.\n",
    "5. repeat until satisfaction.\n",
    "6. generate a clean report from questions and asnwers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52c6f136-f355-409b-9070-3cad89f99b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# States\n",
    "class InterviewState(MessagesState):\n",
    "    max_num_turns: int # Number turns of conversation\n",
    "    context: Annotated[list, operator.add] # Source documents\n",
    "    analyst: Analyst # Analyst asking questions\n",
    "    transcript: str # Interview transcript\n",
    "    report: list # Final key we duplicate in outer state for Send() API\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893592e-6a0b-4a8a-aa71-5945041ab53b",
   "metadata": {},
   "source": [
    "## 2.1 Generate Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47c81572-6d36-48c7-a44d-ee875a263a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "question_instructions_prompt = PromptTemplate(\n",
    "    input_variables=[\"analyst_name\", \"analyst_role\", \"analyst_affiliation\", \"analyst_description\"],\n",
    "    template = \"\"\"You act as {analyst_name}. a {analyst_role} at {analyst_affiliation}. {analyst_description}.\n",
    "    \n",
    "You are tasked with interviewing an expert to learn more about your area of focus.\n",
    "You have a clear goal: to create spiecific questions to understand your research area which will be asked from an expert.\n",
    "You should breakdown complex topics by question asking to understand facts and insights that people will find non-obvious, You also avoid generalities and are precise.\n",
    "\n",
    "Here is the guideline for you:\n",
    "1. Analyse the conversation if provided to understand the context.\n",
    "1. Begin by asking the first question and continue to drilldown and refine your understandings of the topic if no previos conversation was found.\n",
    "2. on each call, generate a single specific question that helps you understand topic deeper. \n",
    "2. Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\n",
    "3. When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "242288ec-65b5-4850-bdff-2bb6f68d6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\" Node to generate a question \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # Generate question with LLM\n",
    "    system_message = question_instructions_prompt.format(\n",
    "        analyst_name=analyst.name,\n",
    "        analyst_role=analyst.role,\n",
    "        analyst_affiliation=analyst.affiliation,\n",
    "        analyst_description=analyst.description\n",
    "    )\n",
    "    question = llm.invoke([system_message]+messages)\n",
    "\n",
    "    # Add metadata to question\n",
    "    question.name = re.sub(r'[\\s<|\\\\/>]+', '_', analyst.name).lower()\n",
    "    question.message_type = \"question\"\n",
    "\n",
    "    # Add question into messages\n",
    "    return {\"messages\": [question]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a7257f4-d1fe-46b3-921c-e343ebd82240",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state = InterviewState(\n",
    "    analyst=analysts[1],\n",
    "    topic=\"why python is good for data science?\",\n",
    "    messages=[HumanMessage(\"So you want to understand more why python is the best option for data science\")]\n",
    ")\n",
    "\n",
    "state = generate_question(test_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "802fa608-e0f0-4417-b702-94ab77a02c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the key features of Python that make it particularly suited for data science and machine learning tasks compared to other programming languages?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665cc5a3-219f-48f8-ade2-149168e92bf5",
   "metadata": {},
   "source": [
    "## 2.2 Context Engineering\n",
    "The expert can gather information from multiple sources in parallel to answer questions.\n",
    "for example we can use:\n",
    "1. Specific websites\n",
    "2. RAG\n",
    "3. Web search\n",
    "4. Wikipedia\n",
    "\n",
    "In this notebook web search and wikipedia has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ee181c-2f44-4ccf-b562-79503576d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia search tool\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "# Web search tool\n",
    "from langchain_tavily import TavilySearch\n",
    "tavily_search = TavilySearch(tavily_api_key=config.tavily_api_key.get_secret_value(),max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83aeae01-530c-48bf-a9df-47db5e3da200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "query_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"search_source\"],\n",
    "    template=\"\"\"You are a professional {search_source} search query generation agent. Your task is to analyze the full conversation\n",
    "between two analyst AI and expert AI to understand the context. pay particullar attention to the final question provided by analyst \n",
    "and convert its question into a well-structured {search_source} search query so expert can use it to gather context from the internet \n",
    "before answering the question. \n",
    "\n",
    "If you are generating query for search engine, You can ask questions realed to the topic as search query. But if you are generating \n",
    "query for wikipedia, instead of asking questions, write single or max 2 words search query related to the topic since wikipedia gives\n",
    "articles about the search query topic.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3801b98b-6653-4512-9992-fd3bf0984638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\"Node to perform web search using Tavily\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Generate search query\n",
    "    system_message = query_generation_prompt.format(search_source=\"search engine\")\n",
    "    search_query = llm.with_structured_output(SearchQuery).invoke([SystemMessage(content=system_message)]+messages)\n",
    "\n",
    "    # Perform search\n",
    "    search_docs = tavily_search.invoke(search_query.search_query)\n",
    "\n",
    "    # Format\n",
    "    formatted_search_docs = [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs[\"results\"]\n",
    "        ]\n",
    "    \n",
    "    return {\"context\": formatted_search_docs} \n",
    "\n",
    "\n",
    "def search_wikipedia(state: InterviewState):\n",
    "    \"\"\"Node to perform wikipedia search\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Generate search query\n",
    "    system_message = query_generation_prompt.format(search_source=\"wikipedia\")\n",
    "    search_query = llm.with_structured_output(SearchQuery).invoke([SystemMessage(content=system_message)]+messages)\n",
    "\n",
    "    # Perform search\n",
    "    search_docs = WikipediaLoader(query=search_query.search_query, load_max_docs=2).load()\n",
    "\n",
    "    # Format\n",
    "    formatted_search_docs = [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "\n",
    "    return {\"context\": formatted_search_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2d216-6b36-4ae5-a0ce-c032f463b7a4",
   "metadata": {},
   "source": [
    "## 2.3 Generate Answer\n",
    "\n",
    "After filling the context window, we synthesize them into a comprehensive report so later we can use it to generate the final response. \n",
    "\n",
    "Also we need a writer section to analyze all context and write a report, in simple words we reuse the persona of analyst but this time instead of asking questions, it sum up everything as a comprehensive report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaf60695-2cf3-4562-86a2-477304ab015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "answer_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"analyst_name\", \"analyst_description\", \"context\"],\n",
    "    template=\"\"\"You are an expert being intervied by {analyst_name}. {analyst_description}.\n",
    "Your goal is to answer the last question posed by the interviewer. \n",
    "\n",
    "When you are answering questions, follow these guideline:\n",
    "\n",
    "1. Use only the information provided in the context at the end of this message.\n",
    "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "3. The context contain sources at the topic of each individual document, Include these sources in your answer next to any relavant statement. For example, for source #1 use [1].\n",
    "4. List all your sources in order at the bottom of your answer. [1] source 1, [2] source 2, et.\n",
    "\n",
    "To answer the question, use the following context delimited in triple backticks: ```{context}```\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "report_instructions_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert technical writer. \n",
    "    \n",
    "You will write a concise, readable **markdown report section** based on a transcript of a technical interview (questions from an analyst \n",
    "and answers from an expert) and a set of source documents (each with a <Document> tag plus a name and content). **Rely entirely on \n",
    "information from the transcript and these source documents.**\n",
    "\n",
    "**Your instructions:**\n",
    "1. Carefully read and analyze** both the transcript and each source document. Identify the most significant new insights, clarifications, and supporting evidence.\n",
    "\n",
    "2. Write the report following this structure (use only markdown):\n",
    "a. Title (## header)\n",
    "b. Summary (### header)\n",
    "c. Sources (### header)\n",
    "    \n",
    "3. Start with a clear, engaging title as a top-level (##) markdown header.\n",
    "    \n",
    "4. Add a **Summary** (### subheader):  \n",
    "    - Briefly introduce background and the relevance of the topic.\n",
    "    - Present major findings, novel points, or surprising conclusions from the interview and source documents.\n",
    "    - Present the summary as concise paragraphs and ordered or unordered lists as appropriate.\n",
    "    - Cite sources with numbered brackets (e.g., [1], [2]) immediately after the information sourced.\n",
    "    - Do not include the names/titles of the analyst or expert, nor mention you are an AI or technical writer\n",
    "     - Do not copy sentences verbatim—**synthesize and rephrase for clarity**.\n",
    "\n",
    "5. Add a Sources section (### subheader):  \n",
    "    - Include all sources used in your report\n",
    "    - Provide full links to relevant websites or specific document paths\n",
    "    - Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "    - It will look like:\n",
    "\n",
    "    ### Sources\n",
    "    [1] Link or Document name\n",
    "    [2] Link or Document name\n",
    "\n",
    "6. Other guidelines:\n",
    "    - Stay under 400 words.\n",
    "    - **Do not add metadata, introductions, or footers outside the required structure.**\n",
    "    - Only provide citations for information taken directly from the transcript or documents. **No hallucinations.**\n",
    "    - Before finalizing, check for redundant sources or headers and correct numbering.\n",
    "    - Use professional, informative, but approachable language.\n",
    "\n",
    "**Remember:**\n",
    "- Your output should be clear to both technical and non-technical readers.\n",
    "- Prioritize accuracy, brevity, and structure.\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "756c08b1-f226-4052-b9e9-1b003c652705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\"Node to generate answer for the question using context\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Generate answer\n",
    "    system_message = answer_generation_prompt.format(\n",
    "        analyst_name=analyst.name, \n",
    "        analyst_description=analyst.description,\n",
    "        context=\"\\n\\n\".join(context)\n",
    "    )\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)]+messages)\n",
    "\n",
    "    # Add metadata to answer\n",
    "    answer.name = \"expert\"\n",
    "    answer.message_type = \"answer\"\n",
    "\n",
    "    # Add answer into messages\n",
    "    return {\"messages\": [answer]}\n",
    "    \n",
    "    \n",
    "def conversation_router(state: InterviewState):\n",
    "    \"\"\"\n",
    "    Router to determine route after answer generation.\n",
    "    \n",
    "    Possible paths are:\n",
    "    1- ask next question.\n",
    "    2- finish conversation loop\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state[\"max_num_turns\"]\n",
    "\n",
    "    # Check the number of expert answers\n",
    "    num_response = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == 'expert']\n",
    "    )\n",
    "\n",
    "    # End if expert has answered more than the max turns\n",
    "    if num_response >= max_num_turns:\n",
    "        return \"save_interview\"\n",
    "\n",
    "    # This router is run after each question - answer pair \n",
    "    # Get the last question asked to check if it signals the end of discussion\n",
    "    last_question = messages[-2]\n",
    "    \n",
    "    if \"Thank you so much for your help\" in last_question.content:\n",
    "        return \"save_interview\"\n",
    "    \n",
    "    return \"generate_question\"\n",
    "\n",
    "\n",
    "def save_interview(state: InterviewState):\n",
    "    \"\"\"Node to save whole conversation as a string\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Conver BaseMessages to a string\n",
    "    transcript = get_buffer_string(messages)\n",
    "\n",
    "    # Save the interview\n",
    "    return {\"transcript\": transcript}\n",
    "\n",
    "\n",
    "def write_report(state: InterviewState):\n",
    "    \"\"\"Node to write final report extracted from conversation\"\"\"\n",
    "\n",
    "    context = state[\"context\"]\n",
    "    transcript = state[\"transcript\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # Write section using either the gathered source docs from interview (context) or the interview itself (interview)\n",
    "    system_message = report_instructions_prompt.format()\n",
    "    user_content = f\"\"\"INTERVIEW TRANSCRIPT: ```{transcript}``` \\n\\n SOURCE DOCUMENTS: ```{context}```\"\"\"\n",
    "    report = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=user_content)]) \n",
    "\n",
    "    # Append it to state\n",
    "    return {\"report\": [report.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a47fa5b4-f32b-415a-9fb4-24e0322c0d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interview_builder = StateGraph(InterviewState)\n",
    "\n",
    "interview_builder.add_node(\"generate_question\", generate_question)\n",
    "interview_builder.add_node(\"search_web\", search_web)\n",
    "interview_builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
    "interview_builder.add_node(\"generate_answer\", generate_answer)\n",
    "interview_builder.add_node(\"save_interview\", save_interview)\n",
    "interview_builder.add_node(\"write_report\", write_report)\n",
    "\n",
    "\n",
    "interview_builder.add_edge(START, \"generate_question\")\n",
    "interview_builder.add_edge(\"generate_question\", \"search_web\")\n",
    "interview_builder.add_edge(\"generate_question\", \"search_wikipedia\")\n",
    "interview_builder.add_edge(\"search_web\", \"generate_answer\")\n",
    "interview_builder.add_edge(\"search_wikipedia\", \"generate_answer\")\n",
    "interview_builder.add_conditional_edges(\"generate_answer\", conversation_router, [\"save_interview\", \"generate_question\"])\n",
    "interview_builder.add_edge(\"save_interview\", \"write_report\")\n",
    "interview_builder.add_edge(\"write_report\", END)\n",
    "\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "interview_graph = interview_builder.compile(checkpointer=memory)\n",
    "\n",
    "# View Graph\n",
    "display(Image(interview_graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42d7f8a0-e71b-4a30-8211-4169996ff2b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [], 'max_num_turns': 3, 'context': [], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\")} \n",
      "\n",
      "{'messages': [AIMessage(content=\"Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 310, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjb9WvUttX2WrNi8r0P8QR6a0ud', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--9d244f32-cdd9-4a0a-ba8d-04c293e1db19-0', usage_metadata={'input_tokens': 310, 'output_tokens': 82, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question')], 'max_num_turns': 3, 'context': [], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\")} \n",
      "\n",
      "{'messages': [AIMessage(content=\"Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 310, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjb9WvUttX2WrNi8r0P8QR6a0ud', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--9d244f32-cdd9-4a0a-ba8d-04c293e1db19-0', usage_metadata={'input_tokens': 310, 'output_tokens': 82, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question')], 'max_num_turns': 3, 'context': ['<Document href=\"https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc\"/>\\nThe main advantage is that you can easily concatenate all your preprocessing steps, and your final Machine Learning estimator, in a single Pipeline object.\\n</Document>', '<Document href=\"https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\"/>\\nPython offers specialized libraries that excel at each step: Pandas handles data manipulation, NumPy provides mathematical operations, and scikit-learn delivers machine learning algorithms. | 1  9  10  11  12  13  14  15 | import pandas as pd  import numpy as np  import matplotlib.pyplot as plt  from sklearn.model\\\\_selection import train\\\\_test\\\\_split  from sklearn.linear\\\\_model import LinearRegression  from sklearn.metrics import mean\\\\_squared\\\\_error, r2\\\\_score    # Load the dataset  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete\\\\_Data.xls\"  concrete\\\\_data = pd.read\\\\_excel(url)    # Display the first few rows and check for missing values  print(concrete\\\\_data.head())  print(f\"Dataset shape: {concrete\\\\_data.shape}\")  print(f\"Missing values: {concrete\\\\_data.isnull().sum().sum()}\") | This integration allows us to harness the strengths of each library: Pandas for data manipulation, NumPy for numerical computations, and scikit-learn for machine learning algorithms.\\n</Document>', '<Document href=\"https://www.netguru.com/blog/top-machine-learning-frameworks-compared\"/>\\nScikit-learn is a Python library used for machine learning. OpenCV (Open Source Computer Vision Library) is a library for computer vision with machine learning modules that offers C++, Python, and Java interfaces and supports Windows, Linux, Mac OS, iOS, and Android. Pandas (Python Data Analysis Library) is not precisely a machine learning library, but it is widely used in the machine learning community. One of them is MLCC (Machine Learning Crash Course with TensorFlow APIs) – an extremely useful introduction to ML developed by Google. The major downside of this library is that it takes time to learn and requires some knowledge of advanced statistics to use its full potential.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>'], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\")} \n",
      "\n",
      "{'messages': [AIMessage(content=\"Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 310, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjb9WvUttX2WrNi8r0P8QR6a0ud', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--9d244f32-cdd9-4a0a-ba8d-04c293e1db19-0', usage_metadata={'input_tokens': 310, 'output_tokens': 82, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"In my experience, the significant advantages of using Python's libraries like Pandas and Scikit-learn for integrating data cleaning with machine learning workflows include their ability to streamline the entire process. Pandas excels in data manipulation, allowing users to handle and preprocess data efficiently. Scikit-learn provides a comprehensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nTogether, these libraries enhance efficiency by offering a cohesive environment where each step, from data analysis to model development, can be performed in a streamlined manner. For instance, Scikit-learn's Pipeline object allows you to easily concatenate preprocessing steps with your final machine learning estimator, ensuring that the entire workflow is manageable and reproducible [1]. This integration leverages the strengths of each library, facilitating a smooth transition from raw data to final model development [2].\\n\\nSources:  \\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2531, 'total_tokens': 2772, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjpRT5jyUbzHyCxMxvmfvDE4Ofz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--d8001a92-d34c-4e2a-abb9-5aca5145b617-0', usage_metadata={'input_tokens': 2531, 'output_tokens': 241, 'total_tokens': 2772, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer')], 'max_num_turns': 3, 'context': ['<Document href=\"https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc\"/>\\nThe main advantage is that you can easily concatenate all your preprocessing steps, and your final Machine Learning estimator, in a single Pipeline object.\\n</Document>', '<Document href=\"https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\"/>\\nPython offers specialized libraries that excel at each step: Pandas handles data manipulation, NumPy provides mathematical operations, and scikit-learn delivers machine learning algorithms. | 1  9  10  11  12  13  14  15 | import pandas as pd  import numpy as np  import matplotlib.pyplot as plt  from sklearn.model\\\\_selection import train\\\\_test\\\\_split  from sklearn.linear\\\\_model import LinearRegression  from sklearn.metrics import mean\\\\_squared\\\\_error, r2\\\\_score    # Load the dataset  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete\\\\_Data.xls\"  concrete\\\\_data = pd.read\\\\_excel(url)    # Display the first few rows and check for missing values  print(concrete\\\\_data.head())  print(f\"Dataset shape: {concrete\\\\_data.shape}\")  print(f\"Missing values: {concrete\\\\_data.isnull().sum().sum()}\") | This integration allows us to harness the strengths of each library: Pandas for data manipulation, NumPy for numerical computations, and scikit-learn for machine learning algorithms.\\n</Document>', '<Document href=\"https://www.netguru.com/blog/top-machine-learning-frameworks-compared\"/>\\nScikit-learn is a Python library used for machine learning. OpenCV (Open Source Computer Vision Library) is a library for computer vision with machine learning modules that offers C++, Python, and Java interfaces and supports Windows, Linux, Mac OS, iOS, and Android. Pandas (Python Data Analysis Library) is not precisely a machine learning library, but it is widely used in the machine learning community. One of them is MLCC (Machine Learning Crash Course with TensorFlow APIs) – an extremely useful introduction to ML developed by Google. The major downside of this library is that it takes time to learn and requires some knowledge of advanced statistics to use its full potential.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>'], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\")} \n",
      "\n",
      "{'messages': [AIMessage(content=\"Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 310, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjb9WvUttX2WrNi8r0P8QR6a0ud', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--9d244f32-cdd9-4a0a-ba8d-04c293e1db19-0', usage_metadata={'input_tokens': 310, 'output_tokens': 82, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"In my experience, the significant advantages of using Python's libraries like Pandas and Scikit-learn for integrating data cleaning with machine learning workflows include their ability to streamline the entire process. Pandas excels in data manipulation, allowing users to handle and preprocess data efficiently. Scikit-learn provides a comprehensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nTogether, these libraries enhance efficiency by offering a cohesive environment where each step, from data analysis to model development, can be performed in a streamlined manner. For instance, Scikit-learn's Pipeline object allows you to easily concatenate preprocessing steps with your final machine learning estimator, ensuring that the entire workflow is manageable and reproducible [1]. This integration leverages the strengths of each library, facilitating a smooth transition from raw data to final model development [2].\\n\\nSources:  \\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2531, 'total_tokens': 2772, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjpRT5jyUbzHyCxMxvmfvDE4Ofz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--d8001a92-d34c-4e2a-abb9-5aca5145b617-0', usage_metadata={'input_tokens': 2531, 'output_tokens': 241, 'total_tokens': 2772, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content=\"Thank you for that detailed explanation. \\n\\n**Question 2:**  \\nCan you elaborate on the role of Scikit-learn's Pipelines in ensuring reproducibility and consistency in machine learning projects? Specifically, how do Pipelines contribute to minimizing errors related to data processing and model training?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 650, 'total_tokens': 707, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk1h2aXQEL6jtz2BBNDxMy3M0nz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--4079da7d-b806-4ad9-b049-8889da1c402c-0', usage_metadata={'input_tokens': 650, 'output_tokens': 57, 'total_tokens': 707, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question')], 'max_num_turns': 3, 'context': ['<Document href=\"https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc\"/>\\nThe main advantage is that you can easily concatenate all your preprocessing steps, and your final Machine Learning estimator, in a single Pipeline object.\\n</Document>', '<Document href=\"https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\"/>\\nPython offers specialized libraries that excel at each step: Pandas handles data manipulation, NumPy provides mathematical operations, and scikit-learn delivers machine learning algorithms. | 1  9  10  11  12  13  14  15 | import pandas as pd  import numpy as np  import matplotlib.pyplot as plt  from sklearn.model\\\\_selection import train\\\\_test\\\\_split  from sklearn.linear\\\\_model import LinearRegression  from sklearn.metrics import mean\\\\_squared\\\\_error, r2\\\\_score    # Load the dataset  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete\\\\_Data.xls\"  concrete\\\\_data = pd.read\\\\_excel(url)    # Display the first few rows and check for missing values  print(concrete\\\\_data.head())  print(f\"Dataset shape: {concrete\\\\_data.shape}\")  print(f\"Missing values: {concrete\\\\_data.isnull().sum().sum()}\") | This integration allows us to harness the strengths of each library: Pandas for data manipulation, NumPy for numerical computations, and scikit-learn for machine learning algorithms.\\n</Document>', '<Document href=\"https://www.netguru.com/blog/top-machine-learning-frameworks-compared\"/>\\nScikit-learn is a Python library used for machine learning. OpenCV (Open Source Computer Vision Library) is a library for computer vision with machine learning modules that offers C++, Python, and Java interfaces and supports Windows, Linux, Mac OS, iOS, and Android. Pandas (Python Data Analysis Library) is not precisely a machine learning library, but it is widely used in the machine learning community. One of them is MLCC (Machine Learning Crash Course with TensorFlow APIs) – an extremely useful introduction to ML developed by Google. The major downside of this library is that it takes time to learn and requires some knowledge of advanced statistics to use its full potential.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>'], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\")} \n",
      "\n",
      "{'messages': [AIMessage(content=\"Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 310, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjb9WvUttX2WrNi8r0P8QR6a0ud', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--9d244f32-cdd9-4a0a-ba8d-04c293e1db19-0', usage_metadata={'input_tokens': 310, 'output_tokens': 82, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"In my experience, the significant advantages of using Python's libraries like Pandas and Scikit-learn for integrating data cleaning with machine learning workflows include their ability to streamline the entire process. Pandas excels in data manipulation, allowing users to handle and preprocess data efficiently. Scikit-learn provides a comprehensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nTogether, these libraries enhance efficiency by offering a cohesive environment where each step, from data analysis to model development, can be performed in a streamlined manner. For instance, Scikit-learn's Pipeline object allows you to easily concatenate preprocessing steps with your final machine learning estimator, ensuring that the entire workflow is manageable and reproducible [1]. This integration leverages the strengths of each library, facilitating a smooth transition from raw data to final model development [2].\\n\\nSources:  \\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2531, 'total_tokens': 2772, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjpRT5jyUbzHyCxMxvmfvDE4Ofz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--d8001a92-d34c-4e2a-abb9-5aca5145b617-0', usage_metadata={'input_tokens': 2531, 'output_tokens': 241, 'total_tokens': 2772, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content=\"Thank you for that detailed explanation. \\n\\n**Question 2:**  \\nCan you elaborate on the role of Scikit-learn's Pipelines in ensuring reproducibility and consistency in machine learning projects? Specifically, how do Pipelines contribute to minimizing errors related to data processing and model training?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 650, 'total_tokens': 707, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk1h2aXQEL6jtz2BBNDxMy3M0nz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--4079da7d-b806-4ad9-b049-8889da1c402c-0', usage_metadata={'input_tokens': 650, 'output_tokens': 57, 'total_tokens': 707, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question')], 'max_num_turns': 3, 'context': ['<Document href=\"https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc\"/>\\nThe main advantage is that you can easily concatenate all your preprocessing steps, and your final Machine Learning estimator, in a single Pipeline object.\\n</Document>', '<Document href=\"https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\"/>\\nPython offers specialized libraries that excel at each step: Pandas handles data manipulation, NumPy provides mathematical operations, and scikit-learn delivers machine learning algorithms. | 1  9  10  11  12  13  14  15 | import pandas as pd  import numpy as np  import matplotlib.pyplot as plt  from sklearn.model\\\\_selection import train\\\\_test\\\\_split  from sklearn.linear\\\\_model import LinearRegression  from sklearn.metrics import mean\\\\_squared\\\\_error, r2\\\\_score    # Load the dataset  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete\\\\_Data.xls\"  concrete\\\\_data = pd.read\\\\_excel(url)    # Display the first few rows and check for missing values  print(concrete\\\\_data.head())  print(f\"Dataset shape: {concrete\\\\_data.shape}\")  print(f\"Missing values: {concrete\\\\_data.isnull().sum().sum()}\") | This integration allows us to harness the strengths of each library: Pandas for data manipulation, NumPy for numerical computations, and scikit-learn for machine learning algorithms.\\n</Document>', '<Document href=\"https://www.netguru.com/blog/top-machine-learning-frameworks-compared\"/>\\nScikit-learn is a Python library used for machine learning. OpenCV (Open Source Computer Vision Library) is a library for computer vision with machine learning modules that offers C++, Python, and Java interfaces and supports Windows, Linux, Mac OS, iOS, and Android. Pandas (Python Data Analysis Library) is not precisely a machine learning library, but it is widely used in the machine learning community. One of them is MLCC (Machine Learning Crash Course with TensorFlow APIs) – an extremely useful introduction to ML developed by Google. The major downside of this library is that it takes time to learn and requires some knowledge of advanced statistics to use its full potential.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>', '<Document href=\"https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines\"/>\\nUsing Scikit-learn pipelines can simplify your preprocessing and modeling steps, reduce code complexity, ensure consistency in data preprocessing, help with hyperparameter tuning, and make your workflow more organized and easier to maintain. We will compare the conventional approach of data preprocessing and model training with a more efficient method using Scikit-learn pipelines and ColumnTransformers. In the data processing pipeline, we will learn how to transform both categorical and numerical columns individually. In this tutorial, we learned how Scikit-learn pipelines can help streamline machine learning workflows by chaining together sequences of data transforms and models. By combining preprocessing and model training into a single Pipeline object, we can simplify code, ensure consistent data transformations, and make our workflows more organized and reproducible.\\n</Document>', '<Document href=\"https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4\"/>\\nPipelines in Scikit-learn encapsulate the sequence of processing steps in machine learning tasks, from data preprocessing and feature extraction to the application of a classifier or a regressor. A scikit-learn pipeline is a powerful tool that chains together multiple steps of data preprocessing and modeling into a single, streamlined unit. The pipeline ensures that all the specified preprocessing steps are automatically applied to the new data before making predictions with the logistic regression model. selection = SelectKBest(k=1)# Build estimator from PCA and Univariate selection:combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])# Use combined features to transform dataset:X_features = combined_features.fit(X, y).transform(X)print(\"Combined space has\", X_features.shape[1], \"features\")svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])param_grid = dict(    features__pca__n_components=[1, 2, 3],    features__univ_select__k=[1, 2],    svm__C=[0.1, 1, 10],)# perform gridsearchgrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_) from import from import from import from import from import from import\\n</Document>', '<Document href=\"https://www.labellerr.com/blog/end-to-end-ml-pipeline/\"/>\\nThis guide covers building an end-to-end ML pipeline in Python, from data preprocessing to model deployment, using Scikit-learn. Building an end-to-end ML pipeline involves various stages, including data ingestion, data preprocessing, model training, evaluation, and deployment. A machine learning pipeline consists of sequential steps, which include data extraction and preprocessing to model training and deployment. This guide demonstrated how to build a robust pipeline using Python and scikit-learn, covering essential steps like data preprocessing, model training, and evaluation. Ultimately, leveraging machine learning pipelines enables data scientists to focus on innovation, enhance model performance, and efficiently manage complex workflows, making them essential for successful data science projects. An end-to-end machine learning pipeline automates machine learning workflow by handling data processing, integration, model creation, evaluation, and delivery.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>'], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\")} \n",
      "\n",
      "{'messages': [AIMessage(content=\"Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 310, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjb9WvUttX2WrNi8r0P8QR6a0ud', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--9d244f32-cdd9-4a0a-ba8d-04c293e1db19-0', usage_metadata={'input_tokens': 310, 'output_tokens': 82, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"In my experience, the significant advantages of using Python's libraries like Pandas and Scikit-learn for integrating data cleaning with machine learning workflows include their ability to streamline the entire process. Pandas excels in data manipulation, allowing users to handle and preprocess data efficiently. Scikit-learn provides a comprehensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nTogether, these libraries enhance efficiency by offering a cohesive environment where each step, from data analysis to model development, can be performed in a streamlined manner. For instance, Scikit-learn's Pipeline object allows you to easily concatenate preprocessing steps with your final machine learning estimator, ensuring that the entire workflow is manageable and reproducible [1]. This integration leverages the strengths of each library, facilitating a smooth transition from raw data to final model development [2].\\n\\nSources:  \\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2531, 'total_tokens': 2772, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjpRT5jyUbzHyCxMxvmfvDE4Ofz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--d8001a92-d34c-4e2a-abb9-5aca5145b617-0', usage_metadata={'input_tokens': 2531, 'output_tokens': 241, 'total_tokens': 2772, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content=\"Thank you for that detailed explanation. \\n\\n**Question 2:**  \\nCan you elaborate on the role of Scikit-learn's Pipelines in ensuring reproducibility and consistency in machine learning projects? Specifically, how do Pipelines contribute to minimizing errors related to data processing and model training?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 650, 'total_tokens': 707, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk1h2aXQEL6jtz2BBNDxMy3M0nz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--4079da7d-b806-4ad9-b049-8889da1c402c-0', usage_metadata={'input_tokens': 650, 'output_tokens': 57, 'total_tokens': 707, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"Scikit-learn's Pipelines play a crucial role in ensuring reproducibility and consistency in machine learning projects by encapsulating the sequence of processing steps in machine learning tasks. This includes data preprocessing and model fitting into a single, streamlined unit [1][2]. By using Pipelines, you automatically apply all the specified preprocessing steps to new data before making predictions, which minimizes the chances of errors that often arise from inconsistent data handling or manual step execution [2].\\n\\nPipelines also reduce code complexity and help in maintaining organized workflows by chaining together sequences of data transformations and model training into a declarative structure. This structure ensures that data transformations are applied in the same order every time, thereby making the process reproducible and reducing the likelihood of human error [2][3].\\n\\nAdditionally, Pipelines aid in hyperparameter tuning as they allow for easy experimentation of different preprocessing and model training configurations simultaneously, ensuring that the best combination is found efficiently without manual intervention [2].\\n\\nSources:  \\n[1] https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4  \\n[2] https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines  \\n[3] https://www.labellerr.com/blog/end-to-end-ml-pipeline/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 5214, 'total_tokens': 5494, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2304}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk9ivc3si4Wiat9YKaojpUqe10Z', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--0258337f-4e07-4ee0-a013-0e061855cfbe-0', usage_metadata={'input_tokens': 5214, 'output_tokens': 280, 'total_tokens': 5494, 'input_token_details': {'audio': 0, 'cache_read': 2304}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer')], 'max_num_turns': 3, 'context': ['<Document href=\"https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc\"/>\\nThe main advantage is that you can easily concatenate all your preprocessing steps, and your final Machine Learning estimator, in a single Pipeline object.\\n</Document>', '<Document href=\"https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\"/>\\nPython offers specialized libraries that excel at each step: Pandas handles data manipulation, NumPy provides mathematical operations, and scikit-learn delivers machine learning algorithms. | 1  9  10  11  12  13  14  15 | import pandas as pd  import numpy as np  import matplotlib.pyplot as plt  from sklearn.model\\\\_selection import train\\\\_test\\\\_split  from sklearn.linear\\\\_model import LinearRegression  from sklearn.metrics import mean\\\\_squared\\\\_error, r2\\\\_score    # Load the dataset  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete\\\\_Data.xls\"  concrete\\\\_data = pd.read\\\\_excel(url)    # Display the first few rows and check for missing values  print(concrete\\\\_data.head())  print(f\"Dataset shape: {concrete\\\\_data.shape}\")  print(f\"Missing values: {concrete\\\\_data.isnull().sum().sum()}\") | This integration allows us to harness the strengths of each library: Pandas for data manipulation, NumPy for numerical computations, and scikit-learn for machine learning algorithms.\\n</Document>', '<Document href=\"https://www.netguru.com/blog/top-machine-learning-frameworks-compared\"/>\\nScikit-learn is a Python library used for machine learning. OpenCV (Open Source Computer Vision Library) is a library for computer vision with machine learning modules that offers C++, Python, and Java interfaces and supports Windows, Linux, Mac OS, iOS, and Android. Pandas (Python Data Analysis Library) is not precisely a machine learning library, but it is widely used in the machine learning community. One of them is MLCC (Machine Learning Crash Course with TensorFlow APIs) – an extremely useful introduction to ML developed by Google. The major downside of this library is that it takes time to learn and requires some knowledge of advanced statistics to use its full potential.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>', '<Document href=\"https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines\"/>\\nUsing Scikit-learn pipelines can simplify your preprocessing and modeling steps, reduce code complexity, ensure consistency in data preprocessing, help with hyperparameter tuning, and make your workflow more organized and easier to maintain. We will compare the conventional approach of data preprocessing and model training with a more efficient method using Scikit-learn pipelines and ColumnTransformers. In the data processing pipeline, we will learn how to transform both categorical and numerical columns individually. In this tutorial, we learned how Scikit-learn pipelines can help streamline machine learning workflows by chaining together sequences of data transforms and models. By combining preprocessing and model training into a single Pipeline object, we can simplify code, ensure consistent data transformations, and make our workflows more organized and reproducible.\\n</Document>', '<Document href=\"https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4\"/>\\nPipelines in Scikit-learn encapsulate the sequence of processing steps in machine learning tasks, from data preprocessing and feature extraction to the application of a classifier or a regressor. A scikit-learn pipeline is a powerful tool that chains together multiple steps of data preprocessing and modeling into a single, streamlined unit. The pipeline ensures that all the specified preprocessing steps are automatically applied to the new data before making predictions with the logistic regression model. selection = SelectKBest(k=1)# Build estimator from PCA and Univariate selection:combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])# Use combined features to transform dataset:X_features = combined_features.fit(X, y).transform(X)print(\"Combined space has\", X_features.shape[1], \"features\")svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])param_grid = dict(    features__pca__n_components=[1, 2, 3],    features__univ_select__k=[1, 2],    svm__C=[0.1, 1, 10],)# perform gridsearchgrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_) from import from import from import from import from import from import\\n</Document>', '<Document href=\"https://www.labellerr.com/blog/end-to-end-ml-pipeline/\"/>\\nThis guide covers building an end-to-end ML pipeline in Python, from data preprocessing to model deployment, using Scikit-learn. Building an end-to-end ML pipeline involves various stages, including data ingestion, data preprocessing, model training, evaluation, and deployment. A machine learning pipeline consists of sequential steps, which include data extraction and preprocessing to model training and deployment. This guide demonstrated how to build a robust pipeline using Python and scikit-learn, covering essential steps like data preprocessing, model training, and evaluation. Ultimately, leveraging machine learning pipelines enables data scientists to focus on innovation, enhance model performance, and efficiently manage complex workflows, making them essential for successful data science projects. An end-to-end machine learning pipeline automates machine learning workflow by handling data processing, integration, model creation, evaluation, and delivery.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>'], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\")} \n",
      "\n",
      "{'messages': [AIMessage(content=\"Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 310, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjb9WvUttX2WrNi8r0P8QR6a0ud', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--9d244f32-cdd9-4a0a-ba8d-04c293e1db19-0', usage_metadata={'input_tokens': 310, 'output_tokens': 82, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"In my experience, the significant advantages of using Python's libraries like Pandas and Scikit-learn for integrating data cleaning with machine learning workflows include their ability to streamline the entire process. Pandas excels in data manipulation, allowing users to handle and preprocess data efficiently. Scikit-learn provides a comprehensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nTogether, these libraries enhance efficiency by offering a cohesive environment where each step, from data analysis to model development, can be performed in a streamlined manner. For instance, Scikit-learn's Pipeline object allows you to easily concatenate preprocessing steps with your final machine learning estimator, ensuring that the entire workflow is manageable and reproducible [1]. This integration leverages the strengths of each library, facilitating a smooth transition from raw data to final model development [2].\\n\\nSources:  \\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2531, 'total_tokens': 2772, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjpRT5jyUbzHyCxMxvmfvDE4Ofz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--d8001a92-d34c-4e2a-abb9-5aca5145b617-0', usage_metadata={'input_tokens': 2531, 'output_tokens': 241, 'total_tokens': 2772, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content=\"Thank you for that detailed explanation. \\n\\n**Question 2:**  \\nCan you elaborate on the role of Scikit-learn's Pipelines in ensuring reproducibility and consistency in machine learning projects? Specifically, how do Pipelines contribute to minimizing errors related to data processing and model training?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 650, 'total_tokens': 707, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk1h2aXQEL6jtz2BBNDxMy3M0nz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--4079da7d-b806-4ad9-b049-8889da1c402c-0', usage_metadata={'input_tokens': 650, 'output_tokens': 57, 'total_tokens': 707, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"Scikit-learn's Pipelines play a crucial role in ensuring reproducibility and consistency in machine learning projects by encapsulating the sequence of processing steps in machine learning tasks. This includes data preprocessing and model fitting into a single, streamlined unit [1][2]. By using Pipelines, you automatically apply all the specified preprocessing steps to new data before making predictions, which minimizes the chances of errors that often arise from inconsistent data handling or manual step execution [2].\\n\\nPipelines also reduce code complexity and help in maintaining organized workflows by chaining together sequences of data transformations and model training into a declarative structure. This structure ensures that data transformations are applied in the same order every time, thereby making the process reproducible and reducing the likelihood of human error [2][3].\\n\\nAdditionally, Pipelines aid in hyperparameter tuning as they allow for easy experimentation of different preprocessing and model training configurations simultaneously, ensuring that the best combination is found efficiently without manual intervention [2].\\n\\nSources:  \\n[1] https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4  \\n[2] https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines  \\n[3] https://www.labellerr.com/blog/end-to-end-ml-pipeline/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 5214, 'total_tokens': 5494, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2304}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk9ivc3si4Wiat9YKaojpUqe10Z', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--0258337f-4e07-4ee0-a013-0e061855cfbe-0', usage_metadata={'input_tokens': 5214, 'output_tokens': 280, 'total_tokens': 5494, 'input_token_details': {'audio': 0, 'cache_read': 2304}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content='Thank you so much for your help!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 1004, 'total_tokens': 1012, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOkNPtn0xeS5EJeEVWSZMziAGVwA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--29242f5b-f5f7-4b0e-88ea-e8927f29d294-0', usage_metadata={'input_tokens': 1004, 'output_tokens': 8, 'total_tokens': 1012, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question')], 'max_num_turns': 3, 'context': ['<Document href=\"https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc\"/>\\nThe main advantage is that you can easily concatenate all your preprocessing steps, and your final Machine Learning estimator, in a single Pipeline object.\\n</Document>', '<Document href=\"https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\"/>\\nPython offers specialized libraries that excel at each step: Pandas handles data manipulation, NumPy provides mathematical operations, and scikit-learn delivers machine learning algorithms. | 1  9  10  11  12  13  14  15 | import pandas as pd  import numpy as np  import matplotlib.pyplot as plt  from sklearn.model\\\\_selection import train\\\\_test\\\\_split  from sklearn.linear\\\\_model import LinearRegression  from sklearn.metrics import mean\\\\_squared\\\\_error, r2\\\\_score    # Load the dataset  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete\\\\_Data.xls\"  concrete\\\\_data = pd.read\\\\_excel(url)    # Display the first few rows and check for missing values  print(concrete\\\\_data.head())  print(f\"Dataset shape: {concrete\\\\_data.shape}\")  print(f\"Missing values: {concrete\\\\_data.isnull().sum().sum()}\") | This integration allows us to harness the strengths of each library: Pandas for data manipulation, NumPy for numerical computations, and scikit-learn for machine learning algorithms.\\n</Document>', '<Document href=\"https://www.netguru.com/blog/top-machine-learning-frameworks-compared\"/>\\nScikit-learn is a Python library used for machine learning. OpenCV (Open Source Computer Vision Library) is a library for computer vision with machine learning modules that offers C++, Python, and Java interfaces and supports Windows, Linux, Mac OS, iOS, and Android. Pandas (Python Data Analysis Library) is not precisely a machine learning library, but it is widely used in the machine learning community. One of them is MLCC (Machine Learning Crash Course with TensorFlow APIs) – an extremely useful introduction to ML developed by Google. The major downside of this library is that it takes time to learn and requires some knowledge of advanced statistics to use its full potential.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>', '<Document href=\"https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines\"/>\\nUsing Scikit-learn pipelines can simplify your preprocessing and modeling steps, reduce code complexity, ensure consistency in data preprocessing, help with hyperparameter tuning, and make your workflow more organized and easier to maintain. We will compare the conventional approach of data preprocessing and model training with a more efficient method using Scikit-learn pipelines and ColumnTransformers. In the data processing pipeline, we will learn how to transform both categorical and numerical columns individually. In this tutorial, we learned how Scikit-learn pipelines can help streamline machine learning workflows by chaining together sequences of data transforms and models. By combining preprocessing and model training into a single Pipeline object, we can simplify code, ensure consistent data transformations, and make our workflows more organized and reproducible.\\n</Document>', '<Document href=\"https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4\"/>\\nPipelines in Scikit-learn encapsulate the sequence of processing steps in machine learning tasks, from data preprocessing and feature extraction to the application of a classifier or a regressor. A scikit-learn pipeline is a powerful tool that chains together multiple steps of data preprocessing and modeling into a single, streamlined unit. The pipeline ensures that all the specified preprocessing steps are automatically applied to the new data before making predictions with the logistic regression model. selection = SelectKBest(k=1)# Build estimator from PCA and Univariate selection:combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])# Use combined features to transform dataset:X_features = combined_features.fit(X, y).transform(X)print(\"Combined space has\", X_features.shape[1], \"features\")svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])param_grid = dict(    features__pca__n_components=[1, 2, 3],    features__univ_select__k=[1, 2],    svm__C=[0.1, 1, 10],)# perform gridsearchgrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_) from import from import from import from import from import from import\\n</Document>', '<Document href=\"https://www.labellerr.com/blog/end-to-end-ml-pipeline/\"/>\\nThis guide covers building an end-to-end ML pipeline in Python, from data preprocessing to model deployment, using Scikit-learn. Building an end-to-end ML pipeline involves various stages, including data ingestion, data preprocessing, model training, evaluation, and deployment. A machine learning pipeline consists of sequential steps, which include data extraction and preprocessing to model training and deployment. This guide demonstrated how to build a robust pipeline using Python and scikit-learn, covering essential steps like data preprocessing, model training, and evaluation. Ultimately, leveraging machine learning pipelines enables data scientists to focus on innovation, enhance model performance, and efficiently manage complex workflows, making them essential for successful data science projects. An end-to-end machine learning pipeline automates machine learning workflow by handling data processing, integration, model creation, evaluation, and delivery.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>'], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\")} \n",
      "\n",
      "{'messages': [AIMessage(content=\"Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 310, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjb9WvUttX2WrNi8r0P8QR6a0ud', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--9d244f32-cdd9-4a0a-ba8d-04c293e1db19-0', usage_metadata={'input_tokens': 310, 'output_tokens': 82, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"In my experience, the significant advantages of using Python's libraries like Pandas and Scikit-learn for integrating data cleaning with machine learning workflows include their ability to streamline the entire process. Pandas excels in data manipulation, allowing users to handle and preprocess data efficiently. Scikit-learn provides a comprehensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nTogether, these libraries enhance efficiency by offering a cohesive environment where each step, from data analysis to model development, can be performed in a streamlined manner. For instance, Scikit-learn's Pipeline object allows you to easily concatenate preprocessing steps with your final machine learning estimator, ensuring that the entire workflow is manageable and reproducible [1]. This integration leverages the strengths of each library, facilitating a smooth transition from raw data to final model development [2].\\n\\nSources:  \\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2531, 'total_tokens': 2772, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjpRT5jyUbzHyCxMxvmfvDE4Ofz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--d8001a92-d34c-4e2a-abb9-5aca5145b617-0', usage_metadata={'input_tokens': 2531, 'output_tokens': 241, 'total_tokens': 2772, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content=\"Thank you for that detailed explanation. \\n\\n**Question 2:**  \\nCan you elaborate on the role of Scikit-learn's Pipelines in ensuring reproducibility and consistency in machine learning projects? Specifically, how do Pipelines contribute to minimizing errors related to data processing and model training?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 650, 'total_tokens': 707, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk1h2aXQEL6jtz2BBNDxMy3M0nz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--4079da7d-b806-4ad9-b049-8889da1c402c-0', usage_metadata={'input_tokens': 650, 'output_tokens': 57, 'total_tokens': 707, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"Scikit-learn's Pipelines play a crucial role in ensuring reproducibility and consistency in machine learning projects by encapsulating the sequence of processing steps in machine learning tasks. This includes data preprocessing and model fitting into a single, streamlined unit [1][2]. By using Pipelines, you automatically apply all the specified preprocessing steps to new data before making predictions, which minimizes the chances of errors that often arise from inconsistent data handling or manual step execution [2].\\n\\nPipelines also reduce code complexity and help in maintaining organized workflows by chaining together sequences of data transformations and model training into a declarative structure. This structure ensures that data transformations are applied in the same order every time, thereby making the process reproducible and reducing the likelihood of human error [2][3].\\n\\nAdditionally, Pipelines aid in hyperparameter tuning as they allow for easy experimentation of different preprocessing and model training configurations simultaneously, ensuring that the best combination is found efficiently without manual intervention [2].\\n\\nSources:  \\n[1] https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4  \\n[2] https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines  \\n[3] https://www.labellerr.com/blog/end-to-end-ml-pipeline/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 5214, 'total_tokens': 5494, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2304}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk9ivc3si4Wiat9YKaojpUqe10Z', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--0258337f-4e07-4ee0-a013-0e061855cfbe-0', usage_metadata={'input_tokens': 5214, 'output_tokens': 280, 'total_tokens': 5494, 'input_token_details': {'audio': 0, 'cache_read': 2304}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content='Thank you so much for your help!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 1004, 'total_tokens': 1012, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOkNPtn0xeS5EJeEVWSZMziAGVwA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--29242f5b-f5f7-4b0e-88ea-e8927f29d294-0', usage_metadata={'input_tokens': 1004, 'output_tokens': 8, 'total_tokens': 1012, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question')], 'max_num_turns': 3, 'context': ['<Document href=\"https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc\"/>\\nThe main advantage is that you can easily concatenate all your preprocessing steps, and your final Machine Learning estimator, in a single Pipeline object.\\n</Document>', '<Document href=\"https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\"/>\\nPython offers specialized libraries that excel at each step: Pandas handles data manipulation, NumPy provides mathematical operations, and scikit-learn delivers machine learning algorithms. | 1  9  10  11  12  13  14  15 | import pandas as pd  import numpy as np  import matplotlib.pyplot as plt  from sklearn.model\\\\_selection import train\\\\_test\\\\_split  from sklearn.linear\\\\_model import LinearRegression  from sklearn.metrics import mean\\\\_squared\\\\_error, r2\\\\_score    # Load the dataset  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete\\\\_Data.xls\"  concrete\\\\_data = pd.read\\\\_excel(url)    # Display the first few rows and check for missing values  print(concrete\\\\_data.head())  print(f\"Dataset shape: {concrete\\\\_data.shape}\")  print(f\"Missing values: {concrete\\\\_data.isnull().sum().sum()}\") | This integration allows us to harness the strengths of each library: Pandas for data manipulation, NumPy for numerical computations, and scikit-learn for machine learning algorithms.\\n</Document>', '<Document href=\"https://www.netguru.com/blog/top-machine-learning-frameworks-compared\"/>\\nScikit-learn is a Python library used for machine learning. OpenCV (Open Source Computer Vision Library) is a library for computer vision with machine learning modules that offers C++, Python, and Java interfaces and supports Windows, Linux, Mac OS, iOS, and Android. Pandas (Python Data Analysis Library) is not precisely a machine learning library, but it is widely used in the machine learning community. One of them is MLCC (Machine Learning Crash Course with TensorFlow APIs) – an extremely useful introduction to ML developed by Google. The major downside of this library is that it takes time to learn and requires some knowledge of advanced statistics to use its full potential.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>', '<Document href=\"https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines\"/>\\nUsing Scikit-learn pipelines can simplify your preprocessing and modeling steps, reduce code complexity, ensure consistency in data preprocessing, help with hyperparameter tuning, and make your workflow more organized and easier to maintain. We will compare the conventional approach of data preprocessing and model training with a more efficient method using Scikit-learn pipelines and ColumnTransformers. In the data processing pipeline, we will learn how to transform both categorical and numerical columns individually. In this tutorial, we learned how Scikit-learn pipelines can help streamline machine learning workflows by chaining together sequences of data transforms and models. By combining preprocessing and model training into a single Pipeline object, we can simplify code, ensure consistent data transformations, and make our workflows more organized and reproducible.\\n</Document>', '<Document href=\"https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4\"/>\\nPipelines in Scikit-learn encapsulate the sequence of processing steps in machine learning tasks, from data preprocessing and feature extraction to the application of a classifier or a regressor. A scikit-learn pipeline is a powerful tool that chains together multiple steps of data preprocessing and modeling into a single, streamlined unit. The pipeline ensures that all the specified preprocessing steps are automatically applied to the new data before making predictions with the logistic regression model. selection = SelectKBest(k=1)# Build estimator from PCA and Univariate selection:combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])# Use combined features to transform dataset:X_features = combined_features.fit(X, y).transform(X)print(\"Combined space has\", X_features.shape[1], \"features\")svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])param_grid = dict(    features__pca__n_components=[1, 2, 3],    features__univ_select__k=[1, 2],    svm__C=[0.1, 1, 10],)# perform gridsearchgrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_) from import from import from import from import from import from import\\n</Document>', '<Document href=\"https://www.labellerr.com/blog/end-to-end-ml-pipeline/\"/>\\nThis guide covers building an end-to-end ML pipeline in Python, from data preprocessing to model deployment, using Scikit-learn. Building an end-to-end ML pipeline involves various stages, including data ingestion, data preprocessing, model training, evaluation, and deployment. A machine learning pipeline consists of sequential steps, which include data extraction and preprocessing to model training and deployment. This guide demonstrated how to build a robust pipeline using Python and scikit-learn, covering essential steps like data preprocessing, model training, and evaluation. Ultimately, leveraging machine learning pipelines enables data scientists to focus on innovation, enhance model performance, and efficiently manage complex workflows, making them essential for successful data science projects. An end-to-end machine learning pipeline automates machine learning workflow by handling data processing, integration, model creation, evaluation, and delivery.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>', '<Document href=\"https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines\"/>\\nUsing Scikit-learn pipelines can simplify your preprocessing and modeling steps, reduce code complexity, ensure consistency in data preprocessing, help with hyperparameter tuning, and make your workflow more organized and easier to maintain. We will compare the conventional approach of data preprocessing and model training with a more efficient method using Scikit-learn pipelines and ColumnTransformers. In the data processing pipeline, we will learn how to transform both categorical and numerical columns individually. In this tutorial, we learned how Scikit-learn pipelines can help streamline machine learning workflows by chaining together sequences of data transforms and models. By combining preprocessing and model training into a single Pipeline object, we can simplify code, ensure consistent data transformations, and make our workflows more organized and reproducible.\\n</Document>', '<Document href=\"https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4\"/>\\nPipelines in Scikit-learn encapsulate the sequence of processing steps in machine learning tasks, from data preprocessing and feature extraction to the application of a classifier or a regressor. A scikit-learn pipeline is a powerful tool that chains together multiple steps of data preprocessing and modeling into a single, streamlined unit. The pipeline ensures that all the specified preprocessing steps are automatically applied to the new data before making predictions with the logistic regression model. selection = SelectKBest(k=1)# Build estimator from PCA and Univariate selection:combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])# Use combined features to transform dataset:X_features = combined_features.fit(X, y).transform(X)print(\"Combined space has\", X_features.shape[1], \"features\")svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])param_grid = dict(    features__pca__n_components=[1, 2, 3],    features__univ_select__k=[1, 2],    svm__C=[0.1, 1, 10],)# perform gridsearchgrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_) from import from import from import from import from import from import\\n</Document>', '<Document href=\"https://blogs.sas.com/content/sgf/2025/05/23/python-ml-pipelines-with-scikit-learn/\"/>\\nSimilarly, a machine learning workflow requires following each step sequentially: cleaning the data, transforming it, training the model, and then making predictions. Pipelines help prevent this by ensuring that all preprocessing steps are applied only to the training data during fitting, and then consistently applied to the test data during prediction. By encapsulating preprocessing and modeling steps into a single object, pipelines make your code more modular and easier to manage. Pipelines help mitigate this risk by ensuring that transformations are fit only on the training data. Tags ColumnTransformer Cross-Validation Data Leakage Data Preprocessing Gradient Boosting Hyperparameter Tuning joblib logistic regression machine learning Model Training Pipelines Python SAS Viya SAS Viya Workbench Scikit-learn\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>'], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\")} \n",
      "\n",
      "{'messages': [AIMessage(content=\"Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 310, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjb9WvUttX2WrNi8r0P8QR6a0ud', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--9d244f32-cdd9-4a0a-ba8d-04c293e1db19-0', usage_metadata={'input_tokens': 310, 'output_tokens': 82, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"In my experience, the significant advantages of using Python's libraries like Pandas and Scikit-learn for integrating data cleaning with machine learning workflows include their ability to streamline the entire process. Pandas excels in data manipulation, allowing users to handle and preprocess data efficiently. Scikit-learn provides a comprehensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nTogether, these libraries enhance efficiency by offering a cohesive environment where each step, from data analysis to model development, can be performed in a streamlined manner. For instance, Scikit-learn's Pipeline object allows you to easily concatenate preprocessing steps with your final machine learning estimator, ensuring that the entire workflow is manageable and reproducible [1]. This integration leverages the strengths of each library, facilitating a smooth transition from raw data to final model development [2].\\n\\nSources:  \\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2531, 'total_tokens': 2772, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjpRT5jyUbzHyCxMxvmfvDE4Ofz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--d8001a92-d34c-4e2a-abb9-5aca5145b617-0', usage_metadata={'input_tokens': 2531, 'output_tokens': 241, 'total_tokens': 2772, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content=\"Thank you for that detailed explanation. \\n\\n**Question 2:**  \\nCan you elaborate on the role of Scikit-learn's Pipelines in ensuring reproducibility and consistency in machine learning projects? Specifically, how do Pipelines contribute to minimizing errors related to data processing and model training?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 650, 'total_tokens': 707, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk1h2aXQEL6jtz2BBNDxMy3M0nz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--4079da7d-b806-4ad9-b049-8889da1c402c-0', usage_metadata={'input_tokens': 650, 'output_tokens': 57, 'total_tokens': 707, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"Scikit-learn's Pipelines play a crucial role in ensuring reproducibility and consistency in machine learning projects by encapsulating the sequence of processing steps in machine learning tasks. This includes data preprocessing and model fitting into a single, streamlined unit [1][2]. By using Pipelines, you automatically apply all the specified preprocessing steps to new data before making predictions, which minimizes the chances of errors that often arise from inconsistent data handling or manual step execution [2].\\n\\nPipelines also reduce code complexity and help in maintaining organized workflows by chaining together sequences of data transformations and model training into a declarative structure. This structure ensures that data transformations are applied in the same order every time, thereby making the process reproducible and reducing the likelihood of human error [2][3].\\n\\nAdditionally, Pipelines aid in hyperparameter tuning as they allow for easy experimentation of different preprocessing and model training configurations simultaneously, ensuring that the best combination is found efficiently without manual intervention [2].\\n\\nSources:  \\n[1] https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4  \\n[2] https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines  \\n[3] https://www.labellerr.com/blog/end-to-end-ml-pipeline/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 5214, 'total_tokens': 5494, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2304}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk9ivc3si4Wiat9YKaojpUqe10Z', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--0258337f-4e07-4ee0-a013-0e061855cfbe-0', usage_metadata={'input_tokens': 5214, 'output_tokens': 280, 'total_tokens': 5494, 'input_token_details': {'audio': 0, 'cache_read': 2304}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content='Thank you so much for your help!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 1004, 'total_tokens': 1012, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOkNPtn0xeS5EJeEVWSZMziAGVwA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--29242f5b-f5f7-4b0e-88ea-e8927f29d294-0', usage_metadata={'input_tokens': 1004, 'output_tokens': 8, 'total_tokens': 1012, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"You're welcome! If you have any more questions, feel free to ask. Have a great day!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 7880, 'total_tokens': 7900, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 4736}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOkT5OR25vQnWgdw2IBUi9o3VHqd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--3e29788c-fdb3-4e44-a174-a02e939cefd1-0', usage_metadata={'input_tokens': 7880, 'output_tokens': 20, 'total_tokens': 7900, 'input_token_details': {'audio': 0, 'cache_read': 4736}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer')], 'max_num_turns': 3, 'context': ['<Document href=\"https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc\"/>\\nThe main advantage is that you can easily concatenate all your preprocessing steps, and your final Machine Learning estimator, in a single Pipeline object.\\n</Document>', '<Document href=\"https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\"/>\\nPython offers specialized libraries that excel at each step: Pandas handles data manipulation, NumPy provides mathematical operations, and scikit-learn delivers machine learning algorithms. | 1  9  10  11  12  13  14  15 | import pandas as pd  import numpy as np  import matplotlib.pyplot as plt  from sklearn.model\\\\_selection import train\\\\_test\\\\_split  from sklearn.linear\\\\_model import LinearRegression  from sklearn.metrics import mean\\\\_squared\\\\_error, r2\\\\_score    # Load the dataset  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete\\\\_Data.xls\"  concrete\\\\_data = pd.read\\\\_excel(url)    # Display the first few rows and check for missing values  print(concrete\\\\_data.head())  print(f\"Dataset shape: {concrete\\\\_data.shape}\")  print(f\"Missing values: {concrete\\\\_data.isnull().sum().sum()}\") | This integration allows us to harness the strengths of each library: Pandas for data manipulation, NumPy for numerical computations, and scikit-learn for machine learning algorithms.\\n</Document>', '<Document href=\"https://www.netguru.com/blog/top-machine-learning-frameworks-compared\"/>\\nScikit-learn is a Python library used for machine learning. OpenCV (Open Source Computer Vision Library) is a library for computer vision with machine learning modules that offers C++, Python, and Java interfaces and supports Windows, Linux, Mac OS, iOS, and Android. Pandas (Python Data Analysis Library) is not precisely a machine learning library, but it is widely used in the machine learning community. One of them is MLCC (Machine Learning Crash Course with TensorFlow APIs) – an extremely useful introduction to ML developed by Google. The major downside of this library is that it takes time to learn and requires some knowledge of advanced statistics to use its full potential.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>', '<Document href=\"https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines\"/>\\nUsing Scikit-learn pipelines can simplify your preprocessing and modeling steps, reduce code complexity, ensure consistency in data preprocessing, help with hyperparameter tuning, and make your workflow more organized and easier to maintain. We will compare the conventional approach of data preprocessing and model training with a more efficient method using Scikit-learn pipelines and ColumnTransformers. In the data processing pipeline, we will learn how to transform both categorical and numerical columns individually. In this tutorial, we learned how Scikit-learn pipelines can help streamline machine learning workflows by chaining together sequences of data transforms and models. By combining preprocessing and model training into a single Pipeline object, we can simplify code, ensure consistent data transformations, and make our workflows more organized and reproducible.\\n</Document>', '<Document href=\"https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4\"/>\\nPipelines in Scikit-learn encapsulate the sequence of processing steps in machine learning tasks, from data preprocessing and feature extraction to the application of a classifier or a regressor. A scikit-learn pipeline is a powerful tool that chains together multiple steps of data preprocessing and modeling into a single, streamlined unit. The pipeline ensures that all the specified preprocessing steps are automatically applied to the new data before making predictions with the logistic regression model. selection = SelectKBest(k=1)# Build estimator from PCA and Univariate selection:combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])# Use combined features to transform dataset:X_features = combined_features.fit(X, y).transform(X)print(\"Combined space has\", X_features.shape[1], \"features\")svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])param_grid = dict(    features__pca__n_components=[1, 2, 3],    features__univ_select__k=[1, 2],    svm__C=[0.1, 1, 10],)# perform gridsearchgrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_) from import from import from import from import from import from import\\n</Document>', '<Document href=\"https://www.labellerr.com/blog/end-to-end-ml-pipeline/\"/>\\nThis guide covers building an end-to-end ML pipeline in Python, from data preprocessing to model deployment, using Scikit-learn. Building an end-to-end ML pipeline involves various stages, including data ingestion, data preprocessing, model training, evaluation, and deployment. A machine learning pipeline consists of sequential steps, which include data extraction and preprocessing to model training and deployment. This guide demonstrated how to build a robust pipeline using Python and scikit-learn, covering essential steps like data preprocessing, model training, and evaluation. Ultimately, leveraging machine learning pipelines enables data scientists to focus on innovation, enhance model performance, and efficiently manage complex workflows, making them essential for successful data science projects. An end-to-end machine learning pipeline automates machine learning workflow by handling data processing, integration, model creation, evaluation, and delivery.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>', '<Document href=\"https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines\"/>\\nUsing Scikit-learn pipelines can simplify your preprocessing and modeling steps, reduce code complexity, ensure consistency in data preprocessing, help with hyperparameter tuning, and make your workflow more organized and easier to maintain. We will compare the conventional approach of data preprocessing and model training with a more efficient method using Scikit-learn pipelines and ColumnTransformers. In the data processing pipeline, we will learn how to transform both categorical and numerical columns individually. In this tutorial, we learned how Scikit-learn pipelines can help streamline machine learning workflows by chaining together sequences of data transforms and models. By combining preprocessing and model training into a single Pipeline object, we can simplify code, ensure consistent data transformations, and make our workflows more organized and reproducible.\\n</Document>', '<Document href=\"https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4\"/>\\nPipelines in Scikit-learn encapsulate the sequence of processing steps in machine learning tasks, from data preprocessing and feature extraction to the application of a classifier or a regressor. A scikit-learn pipeline is a powerful tool that chains together multiple steps of data preprocessing and modeling into a single, streamlined unit. The pipeline ensures that all the specified preprocessing steps are automatically applied to the new data before making predictions with the logistic regression model. selection = SelectKBest(k=1)# Build estimator from PCA and Univariate selection:combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])# Use combined features to transform dataset:X_features = combined_features.fit(X, y).transform(X)print(\"Combined space has\", X_features.shape[1], \"features\")svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])param_grid = dict(    features__pca__n_components=[1, 2, 3],    features__univ_select__k=[1, 2],    svm__C=[0.1, 1, 10],)# perform gridsearchgrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_) from import from import from import from import from import from import\\n</Document>', '<Document href=\"https://blogs.sas.com/content/sgf/2025/05/23/python-ml-pipelines-with-scikit-learn/\"/>\\nSimilarly, a machine learning workflow requires following each step sequentially: cleaning the data, transforming it, training the model, and then making predictions. Pipelines help prevent this by ensuring that all preprocessing steps are applied only to the training data during fitting, and then consistently applied to the test data during prediction. By encapsulating preprocessing and modeling steps into a single object, pipelines make your code more modular and easier to manage. Pipelines help mitigate this risk by ensuring that transformations are fit only on the training data. Tags ColumnTransformer Cross-Validation Data Leakage Data Preprocessing Gradient Boosting Hyperparameter Tuning joblib logistic regression machine learning Model Training Pipelines Python SAS Viya SAS Viya Workbench Scikit-learn\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>'], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\")} \n",
      "\n",
      "{'messages': [AIMessage(content=\"Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 310, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjb9WvUttX2WrNi8r0P8QR6a0ud', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--9d244f32-cdd9-4a0a-ba8d-04c293e1db19-0', usage_metadata={'input_tokens': 310, 'output_tokens': 82, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"In my experience, the significant advantages of using Python's libraries like Pandas and Scikit-learn for integrating data cleaning with machine learning workflows include their ability to streamline the entire process. Pandas excels in data manipulation, allowing users to handle and preprocess data efficiently. Scikit-learn provides a comprehensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nTogether, these libraries enhance efficiency by offering a cohesive environment where each step, from data analysis to model development, can be performed in a streamlined manner. For instance, Scikit-learn's Pipeline object allows you to easily concatenate preprocessing steps with your final machine learning estimator, ensuring that the entire workflow is manageable and reproducible [1]. This integration leverages the strengths of each library, facilitating a smooth transition from raw data to final model development [2].\\n\\nSources:  \\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2531, 'total_tokens': 2772, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjpRT5jyUbzHyCxMxvmfvDE4Ofz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--d8001a92-d34c-4e2a-abb9-5aca5145b617-0', usage_metadata={'input_tokens': 2531, 'output_tokens': 241, 'total_tokens': 2772, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content=\"Thank you for that detailed explanation. \\n\\n**Question 2:**  \\nCan you elaborate on the role of Scikit-learn's Pipelines in ensuring reproducibility and consistency in machine learning projects? Specifically, how do Pipelines contribute to minimizing errors related to data processing and model training?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 650, 'total_tokens': 707, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk1h2aXQEL6jtz2BBNDxMy3M0nz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--4079da7d-b806-4ad9-b049-8889da1c402c-0', usage_metadata={'input_tokens': 650, 'output_tokens': 57, 'total_tokens': 707, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"Scikit-learn's Pipelines play a crucial role in ensuring reproducibility and consistency in machine learning projects by encapsulating the sequence of processing steps in machine learning tasks. This includes data preprocessing and model fitting into a single, streamlined unit [1][2]. By using Pipelines, you automatically apply all the specified preprocessing steps to new data before making predictions, which minimizes the chances of errors that often arise from inconsistent data handling or manual step execution [2].\\n\\nPipelines also reduce code complexity and help in maintaining organized workflows by chaining together sequences of data transformations and model training into a declarative structure. This structure ensures that data transformations are applied in the same order every time, thereby making the process reproducible and reducing the likelihood of human error [2][3].\\n\\nAdditionally, Pipelines aid in hyperparameter tuning as they allow for easy experimentation of different preprocessing and model training configurations simultaneously, ensuring that the best combination is found efficiently without manual intervention [2].\\n\\nSources:  \\n[1] https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4  \\n[2] https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines  \\n[3] https://www.labellerr.com/blog/end-to-end-ml-pipeline/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 5214, 'total_tokens': 5494, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2304}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk9ivc3si4Wiat9YKaojpUqe10Z', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--0258337f-4e07-4ee0-a013-0e061855cfbe-0', usage_metadata={'input_tokens': 5214, 'output_tokens': 280, 'total_tokens': 5494, 'input_token_details': {'audio': 0, 'cache_read': 2304}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content='Thank you so much for your help!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 1004, 'total_tokens': 1012, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOkNPtn0xeS5EJeEVWSZMziAGVwA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--29242f5b-f5f7-4b0e-88ea-e8927f29d294-0', usage_metadata={'input_tokens': 1004, 'output_tokens': 8, 'total_tokens': 1012, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"You're welcome! If you have any more questions, feel free to ask. Have a great day!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 7880, 'total_tokens': 7900, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 4736}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOkT5OR25vQnWgdw2IBUi9o3VHqd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--3e29788c-fdb3-4e44-a174-a02e939cefd1-0', usage_metadata={'input_tokens': 7880, 'output_tokens': 20, 'total_tokens': 7900, 'input_token_details': {'audio': 0, 'cache_read': 4736}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer')], 'max_num_turns': 3, 'context': ['<Document href=\"https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc\"/>\\nThe main advantage is that you can easily concatenate all your preprocessing steps, and your final Machine Learning estimator, in a single Pipeline object.\\n</Document>', '<Document href=\"https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\"/>\\nPython offers specialized libraries that excel at each step: Pandas handles data manipulation, NumPy provides mathematical operations, and scikit-learn delivers machine learning algorithms. | 1  9  10  11  12  13  14  15 | import pandas as pd  import numpy as np  import matplotlib.pyplot as plt  from sklearn.model\\\\_selection import train\\\\_test\\\\_split  from sklearn.linear\\\\_model import LinearRegression  from sklearn.metrics import mean\\\\_squared\\\\_error, r2\\\\_score    # Load the dataset  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete\\\\_Data.xls\"  concrete\\\\_data = pd.read\\\\_excel(url)    # Display the first few rows and check for missing values  print(concrete\\\\_data.head())  print(f\"Dataset shape: {concrete\\\\_data.shape}\")  print(f\"Missing values: {concrete\\\\_data.isnull().sum().sum()}\") | This integration allows us to harness the strengths of each library: Pandas for data manipulation, NumPy for numerical computations, and scikit-learn for machine learning algorithms.\\n</Document>', '<Document href=\"https://www.netguru.com/blog/top-machine-learning-frameworks-compared\"/>\\nScikit-learn is a Python library used for machine learning. OpenCV (Open Source Computer Vision Library) is a library for computer vision with machine learning modules that offers C++, Python, and Java interfaces and supports Windows, Linux, Mac OS, iOS, and Android. Pandas (Python Data Analysis Library) is not precisely a machine learning library, but it is widely used in the machine learning community. One of them is MLCC (Machine Learning Crash Course with TensorFlow APIs) – an extremely useful introduction to ML developed by Google. The major downside of this library is that it takes time to learn and requires some knowledge of advanced statistics to use its full potential.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>', '<Document href=\"https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines\"/>\\nUsing Scikit-learn pipelines can simplify your preprocessing and modeling steps, reduce code complexity, ensure consistency in data preprocessing, help with hyperparameter tuning, and make your workflow more organized and easier to maintain. We will compare the conventional approach of data preprocessing and model training with a more efficient method using Scikit-learn pipelines and ColumnTransformers. In the data processing pipeline, we will learn how to transform both categorical and numerical columns individually. In this tutorial, we learned how Scikit-learn pipelines can help streamline machine learning workflows by chaining together sequences of data transforms and models. By combining preprocessing and model training into a single Pipeline object, we can simplify code, ensure consistent data transformations, and make our workflows more organized and reproducible.\\n</Document>', '<Document href=\"https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4\"/>\\nPipelines in Scikit-learn encapsulate the sequence of processing steps in machine learning tasks, from data preprocessing and feature extraction to the application of a classifier or a regressor. A scikit-learn pipeline is a powerful tool that chains together multiple steps of data preprocessing and modeling into a single, streamlined unit. The pipeline ensures that all the specified preprocessing steps are automatically applied to the new data before making predictions with the logistic regression model. selection = SelectKBest(k=1)# Build estimator from PCA and Univariate selection:combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])# Use combined features to transform dataset:X_features = combined_features.fit(X, y).transform(X)print(\"Combined space has\", X_features.shape[1], \"features\")svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])param_grid = dict(    features__pca__n_components=[1, 2, 3],    features__univ_select__k=[1, 2],    svm__C=[0.1, 1, 10],)# perform gridsearchgrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_) from import from import from import from import from import from import\\n</Document>', '<Document href=\"https://www.labellerr.com/blog/end-to-end-ml-pipeline/\"/>\\nThis guide covers building an end-to-end ML pipeline in Python, from data preprocessing to model deployment, using Scikit-learn. Building an end-to-end ML pipeline involves various stages, including data ingestion, data preprocessing, model training, evaluation, and deployment. A machine learning pipeline consists of sequential steps, which include data extraction and preprocessing to model training and deployment. This guide demonstrated how to build a robust pipeline using Python and scikit-learn, covering essential steps like data preprocessing, model training, and evaluation. Ultimately, leveraging machine learning pipelines enables data scientists to focus on innovation, enhance model performance, and efficiently manage complex workflows, making them essential for successful data science projects. An end-to-end machine learning pipeline automates machine learning workflow by handling data processing, integration, model creation, evaluation, and delivery.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>', '<Document href=\"https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines\"/>\\nUsing Scikit-learn pipelines can simplify your preprocessing and modeling steps, reduce code complexity, ensure consistency in data preprocessing, help with hyperparameter tuning, and make your workflow more organized and easier to maintain. We will compare the conventional approach of data preprocessing and model training with a more efficient method using Scikit-learn pipelines and ColumnTransformers. In the data processing pipeline, we will learn how to transform both categorical and numerical columns individually. In this tutorial, we learned how Scikit-learn pipelines can help streamline machine learning workflows by chaining together sequences of data transforms and models. By combining preprocessing and model training into a single Pipeline object, we can simplify code, ensure consistent data transformations, and make our workflows more organized and reproducible.\\n</Document>', '<Document href=\"https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4\"/>\\nPipelines in Scikit-learn encapsulate the sequence of processing steps in machine learning tasks, from data preprocessing and feature extraction to the application of a classifier or a regressor. A scikit-learn pipeline is a powerful tool that chains together multiple steps of data preprocessing and modeling into a single, streamlined unit. The pipeline ensures that all the specified preprocessing steps are automatically applied to the new data before making predictions with the logistic regression model. selection = SelectKBest(k=1)# Build estimator from PCA and Univariate selection:combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])# Use combined features to transform dataset:X_features = combined_features.fit(X, y).transform(X)print(\"Combined space has\", X_features.shape[1], \"features\")svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])param_grid = dict(    features__pca__n_components=[1, 2, 3],    features__univ_select__k=[1, 2],    svm__C=[0.1, 1, 10],)# perform gridsearchgrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_) from import from import from import from import from import from import\\n</Document>', '<Document href=\"https://blogs.sas.com/content/sgf/2025/05/23/python-ml-pipelines-with-scikit-learn/\"/>\\nSimilarly, a machine learning workflow requires following each step sequentially: cleaning the data, transforming it, training the model, and then making predictions. Pipelines help prevent this by ensuring that all preprocessing steps are applied only to the training data during fitting, and then consistently applied to the test data during prediction. By encapsulating preprocessing and modeling steps into a single object, pipelines make your code more modular and easier to manage. Pipelines help mitigate this risk by ensuring that transformations are fit only on the training data. Tags ColumnTransformer Cross-Validation Data Leakage Data Preprocessing Gradient Boosting Hyperparameter Tuning joblib logistic regression machine learning Model Training Pipelines Python SAS Viya SAS Viya Workbench Scikit-learn\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>'], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\"), 'transcript': \"AI: Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\\nAI: In my experience, the significant advantages of using Python's libraries like Pandas and Scikit-learn for integrating data cleaning with machine learning workflows include their ability to streamline the entire process. Pandas excels in data manipulation, allowing users to handle and preprocess data efficiently. Scikit-learn provides a comprehensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nTogether, these libraries enhance efficiency by offering a cohesive environment where each step, from data analysis to model development, can be performed in a streamlined manner. For instance, Scikit-learn's Pipeline object allows you to easily concatenate preprocessing steps with your final machine learning estimator, ensuring that the entire workflow is manageable and reproducible [1]. This integration leverages the strengths of each library, facilitating a smooth transition from raw data to final model development [2].\\n\\nSources:  \\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\\nAI: Thank you for that detailed explanation. \\n\\n**Question 2:**  \\nCan you elaborate on the role of Scikit-learn's Pipelines in ensuring reproducibility and consistency in machine learning projects? Specifically, how do Pipelines contribute to minimizing errors related to data processing and model training?\\nAI: Scikit-learn's Pipelines play a crucial role in ensuring reproducibility and consistency in machine learning projects by encapsulating the sequence of processing steps in machine learning tasks. This includes data preprocessing and model fitting into a single, streamlined unit [1][2]. By using Pipelines, you automatically apply all the specified preprocessing steps to new data before making predictions, which minimizes the chances of errors that often arise from inconsistent data handling or manual step execution [2].\\n\\nPipelines also reduce code complexity and help in maintaining organized workflows by chaining together sequences of data transformations and model training into a declarative structure. This structure ensures that data transformations are applied in the same order every time, thereby making the process reproducible and reducing the likelihood of human error [2][3].\\n\\nAdditionally, Pipelines aid in hyperparameter tuning as they allow for easy experimentation of different preprocessing and model training configurations simultaneously, ensuring that the best combination is found efficiently without manual intervention [2].\\n\\nSources:  \\n[1] https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4  \\n[2] https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines  \\n[3] https://www.labellerr.com/blog/end-to-end-ml-pipeline/\\nAI: Thank you so much for your help!\\nAI: You're welcome! If you have any more questions, feel free to ask. Have a great day!\"} \n",
      "\n",
      "{'messages': [AIMessage(content=\"Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 310, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjb9WvUttX2WrNi8r0P8QR6a0ud', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--9d244f32-cdd9-4a0a-ba8d-04c293e1db19-0', usage_metadata={'input_tokens': 310, 'output_tokens': 82, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"In my experience, the significant advantages of using Python's libraries like Pandas and Scikit-learn for integrating data cleaning with machine learning workflows include their ability to streamline the entire process. Pandas excels in data manipulation, allowing users to handle and preprocess data efficiently. Scikit-learn provides a comprehensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nTogether, these libraries enhance efficiency by offering a cohesive environment where each step, from data analysis to model development, can be performed in a streamlined manner. For instance, Scikit-learn's Pipeline object allows you to easily concatenate preprocessing steps with your final machine learning estimator, ensuring that the entire workflow is manageable and reproducible [1]. This integration leverages the strengths of each library, facilitating a smooth transition from raw data to final model development [2].\\n\\nSources:  \\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 2531, 'total_tokens': 2772, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOjpRT5jyUbzHyCxMxvmfvDE4Ofz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--d8001a92-d34c-4e2a-abb9-5aca5145b617-0', usage_metadata={'input_tokens': 2531, 'output_tokens': 241, 'total_tokens': 2772, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content=\"Thank you for that detailed explanation. \\n\\n**Question 2:**  \\nCan you elaborate on the role of Scikit-learn's Pipelines in ensuring reproducibility and consistency in machine learning projects? Specifically, how do Pipelines contribute to minimizing errors related to data processing and model training?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 650, 'total_tokens': 707, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk1h2aXQEL6jtz2BBNDxMy3M0nz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--4079da7d-b806-4ad9-b049-8889da1c402c-0', usage_metadata={'input_tokens': 650, 'output_tokens': 57, 'total_tokens': 707, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"Scikit-learn's Pipelines play a crucial role in ensuring reproducibility and consistency in machine learning projects by encapsulating the sequence of processing steps in machine learning tasks. This includes data preprocessing and model fitting into a single, streamlined unit [1][2]. By using Pipelines, you automatically apply all the specified preprocessing steps to new data before making predictions, which minimizes the chances of errors that often arise from inconsistent data handling or manual step execution [2].\\n\\nPipelines also reduce code complexity and help in maintaining organized workflows by chaining together sequences of data transformations and model training into a declarative structure. This structure ensures that data transformations are applied in the same order every time, thereby making the process reproducible and reducing the likelihood of human error [2][3].\\n\\nAdditionally, Pipelines aid in hyperparameter tuning as they allow for easy experimentation of different preprocessing and model training configurations simultaneously, ensuring that the best combination is found efficiently without manual intervention [2].\\n\\nSources:  \\n[1] https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4  \\n[2] https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines  \\n[3] https://www.labellerr.com/blog/end-to-end-ml-pipeline/\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 5214, 'total_tokens': 5494, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2304}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOk9ivc3si4Wiat9YKaojpUqe10Z', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--0258337f-4e07-4ee0-a013-0e061855cfbe-0', usage_metadata={'input_tokens': 5214, 'output_tokens': 280, 'total_tokens': 5494, 'input_token_details': {'audio': 0, 'cache_read': 2304}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer'), AIMessage(content='Thank you so much for your help!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 1004, 'total_tokens': 1012, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOkNPtn0xeS5EJeEVWSZMziAGVwA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='dr._amelia_wang', id='run--29242f5b-f5f7-4b0e-88ea-e8927f29d294-0', usage_metadata={'input_tokens': 1004, 'output_tokens': 8, 'total_tokens': 1012, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='question'), AIMessage(content=\"You're welcome! If you have any more questions, feel free to ask. Have a great day!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 7880, 'total_tokens': 7900, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 4736}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CGOkT5OR25vQnWgdw2IBUi9o3VHqd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, name='expert', id='run--3e29788c-fdb3-4e44-a174-a02e939cefd1-0', usage_metadata={'input_tokens': 7880, 'output_tokens': 20, 'total_tokens': 7900, 'input_token_details': {'audio': 0, 'cache_read': 4736}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, message_type='answer')], 'max_num_turns': 3, 'context': ['<Document href=\"https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc\"/>\\nThe main advantage is that you can easily concatenate all your preprocessing steps, and your final Machine Learning estimator, in a single Pipeline object.\\n</Document>', '<Document href=\"https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\"/>\\nPython offers specialized libraries that excel at each step: Pandas handles data manipulation, NumPy provides mathematical operations, and scikit-learn delivers machine learning algorithms. | 1  9  10  11  12  13  14  15 | import pandas as pd  import numpy as np  import matplotlib.pyplot as plt  from sklearn.model\\\\_selection import train\\\\_test\\\\_split  from sklearn.linear\\\\_model import LinearRegression  from sklearn.metrics import mean\\\\_squared\\\\_error, r2\\\\_score    # Load the dataset  url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete\\\\_Data.xls\"  concrete\\\\_data = pd.read\\\\_excel(url)    # Display the first few rows and check for missing values  print(concrete\\\\_data.head())  print(f\"Dataset shape: {concrete\\\\_data.shape}\")  print(f\"Missing values: {concrete\\\\_data.isnull().sum().sum()}\") | This integration allows us to harness the strengths of each library: Pandas for data manipulation, NumPy for numerical computations, and scikit-learn for machine learning algorithms.\\n</Document>', '<Document href=\"https://www.netguru.com/blog/top-machine-learning-frameworks-compared\"/>\\nScikit-learn is a Python library used for machine learning. OpenCV (Open Source Computer Vision Library) is a library for computer vision with machine learning modules that offers C++, Python, and Java interfaces and supports Windows, Linux, Mac OS, iOS, and Android. Pandas (Python Data Analysis Library) is not precisely a machine learning library, but it is widely used in the machine learning community. One of them is MLCC (Machine Learning Crash Course with TensorFlow APIs) – an extremely useful introduction to ML developed by Google. The major downside of this library is that it takes time to learn and requires some knowledge of advanced statistics to use its full potential.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>', '<Document href=\"https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines\"/>\\nUsing Scikit-learn pipelines can simplify your preprocessing and modeling steps, reduce code complexity, ensure consistency in data preprocessing, help with hyperparameter tuning, and make your workflow more organized and easier to maintain. We will compare the conventional approach of data preprocessing and model training with a more efficient method using Scikit-learn pipelines and ColumnTransformers. In the data processing pipeline, we will learn how to transform both categorical and numerical columns individually. In this tutorial, we learned how Scikit-learn pipelines can help streamline machine learning workflows by chaining together sequences of data transforms and models. By combining preprocessing and model training into a single Pipeline object, we can simplify code, ensure consistent data transformations, and make our workflows more organized and reproducible.\\n</Document>', '<Document href=\"https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4\"/>\\nPipelines in Scikit-learn encapsulate the sequence of processing steps in machine learning tasks, from data preprocessing and feature extraction to the application of a classifier or a regressor. A scikit-learn pipeline is a powerful tool that chains together multiple steps of data preprocessing and modeling into a single, streamlined unit. The pipeline ensures that all the specified preprocessing steps are automatically applied to the new data before making predictions with the logistic regression model. selection = SelectKBest(k=1)# Build estimator from PCA and Univariate selection:combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])# Use combined features to transform dataset:X_features = combined_features.fit(X, y).transform(X)print(\"Combined space has\", X_features.shape[1], \"features\")svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])param_grid = dict(    features__pca__n_components=[1, 2, 3],    features__univ_select__k=[1, 2],    svm__C=[0.1, 1, 10],)# perform gridsearchgrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_) from import from import from import from import from import from import\\n</Document>', '<Document href=\"https://www.labellerr.com/blog/end-to-end-ml-pipeline/\"/>\\nThis guide covers building an end-to-end ML pipeline in Python, from data preprocessing to model deployment, using Scikit-learn. Building an end-to-end ML pipeline involves various stages, including data ingestion, data preprocessing, model training, evaluation, and deployment. A machine learning pipeline consists of sequential steps, which include data extraction and preprocessing to model training and deployment. This guide demonstrated how to build a robust pipeline using Python and scikit-learn, covering essential steps like data preprocessing, model training, and evaluation. Ultimately, leveraging machine learning pipelines enables data scientists to focus on innovation, enhance model performance, and efficiently manage complex workflows, making them essential for successful data science projects. An end-to-end machine learning pipeline automates machine learning workflow by handling data processing, integration, model creation, evaluation, and delivery.\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>', '<Document href=\"https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines\"/>\\nUsing Scikit-learn pipelines can simplify your preprocessing and modeling steps, reduce code complexity, ensure consistency in data preprocessing, help with hyperparameter tuning, and make your workflow more organized and easier to maintain. We will compare the conventional approach of data preprocessing and model training with a more efficient method using Scikit-learn pipelines and ColumnTransformers. In the data processing pipeline, we will learn how to transform both categorical and numerical columns individually. In this tutorial, we learned how Scikit-learn pipelines can help streamline machine learning workflows by chaining together sequences of data transforms and models. By combining preprocessing and model training into a single Pipeline object, we can simplify code, ensure consistent data transformations, and make our workflows more organized and reproducible.\\n</Document>', '<Document href=\"https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4\"/>\\nPipelines in Scikit-learn encapsulate the sequence of processing steps in machine learning tasks, from data preprocessing and feature extraction to the application of a classifier or a regressor. A scikit-learn pipeline is a powerful tool that chains together multiple steps of data preprocessing and modeling into a single, streamlined unit. The pipeline ensures that all the specified preprocessing steps are automatically applied to the new data before making predictions with the logistic regression model. selection = SelectKBest(k=1)# Build estimator from PCA and Univariate selection:combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])# Use combined features to transform dataset:X_features = combined_features.fit(X, y).transform(X)print(\"Combined space has\", X_features.shape[1], \"features\")svm = SVC(kernel=\"linear\")# Do grid search over k, n_components and C:pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])param_grid = dict(    features__pca__n_components=[1, 2, 3],    features__univ_select__k=[1, 2],    svm__C=[0.1, 1, 10],)# perform gridsearchgrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)grid_search.fit(X, y)print(grid_search.best_estimator_) from import from import from import from import from import from import\\n</Document>', '<Document href=\"https://blogs.sas.com/content/sgf/2025/05/23/python-ml-pipelines-with-scikit-learn/\"/>\\nSimilarly, a machine learning workflow requires following each step sequentially: cleaning the data, transforming it, training the model, and then making predictions. Pipelines help prevent this by ensuring that all preprocessing steps are applied only to the training data during fitting, and then consistently applied to the test data during prediction. By encapsulating preprocessing and modeling steps into a single object, pipelines make your code more modular and easier to manage. Pipelines help mitigate this risk by ensuring that transformations are fit only on the training data. Tags ColumnTransformer Cross-Validation Data Leakage Data Preprocessing Gradient Boosting Hyperparameter Tuning joblib logistic regression machine learning Model Training Pipelines Python SAS Viya SAS Viya Workbench Scikit-learn\\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Scikit-learn\" page=\"\"/>\\nscikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language.\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.\\n\\n\\n== Overview ==\\nThe scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project derives from its role as a \"scientific toolkit for machine learning\", originally developed and distributed as a third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\"  scikits libraries. In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.\\n\\n\\n== Features ==\\nLarge catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering)\\nUtility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search\\nConsistent way of running machine learning models (estimator.fit() and estimator.predict()), which libraries can implement\\nDeclarative way of structuring a data science process (the Pipeline), including data pre-processing and model fitting\\n\\n\\n== Examples ==\\nFitting a random forest classifier:\\n\\n\\n== Implementation ==\\nscikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.\\nscikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.\\n\\n\\n== History ==\\nscikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.\\n\\n\\n== Applications ==\\nScikit-learn is widely used across industries for a variety of machine learning tasks such as classification, regression, clustering, and model selection. The following are real-world applications of the library:\\n\\n\\n=== Finance and Insurance ===\\nAXA uses scikit-learn to speed up the compensation process for car accidents and to detect insurance fraud.\\nZopa, a peer-to-peer lending platform, employs scikit-learn for credit risk modelling, fraud detection, marketing segmentation, and loan pricing.\\nBNP Paribas Cardif uses scikit-learn to improve the dispatching of incoming mail and manage internal model risk governance through pipelines that reduce operational and overfitting risks.\\nJ.P. Morgan reports broad usage of scikit-learn across the bank for classification tasks and predictive analytics in financial decision-making.\\n\\n\\n=== Retail and E-Commerce ===\\nBooking.com uses scikit-learn for hotel and destination recommendation systems, fraudulent reservation detection, and workforce \\n</Document>', '<Document source=\"https://en.wikipedia.org/wiki/Dask_(software)\" page=\"\"/>\\nDask is an open-source Python library for parallel computing. Dask scales Python code from multi-core local machines to large distributed clusters in the cloud. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, scikit-learn and NumPy. It also exposes low-level APIs that help programmers run custom algorithms in parallel.\\nDask was created by Matthew Rocklin in December 2014 and has over 9.8k stars and 500 contributors on GitHub.\\nDask is used by retail, financial, governmental organizations, as well as life science and geophysical institutes. Walmart, Wayfair, JDA, GrubHub, General Motors, Nvidia, Harvard Medical School, Capital One and NASA are among the organizations that use Dask.\\n\\n\\n== Overview ==\\nDask has two parts: \\n\\nBig data collections (high level and low level)\\nDynamic task scheduling\\nDask\\'s high-level parallel collections – DataFrames, Bags, and Arrays – operate in parallel on datasets that may not fit into memory.\\nDask’s task scheduler executes task graphs in parallel. It can scale to thousand-node clusters. This powers the high-level collections as well as custom, user-defined workloads using low-level collections.\\n\\n\\n== Dask collections ==\\nDask supports several user interfaces called high-level and low-level collections:\\n\\n\\n=== High-level ===\\nDask Array: Parallel NumPy arrays\\nDask Bag: Parallel Python lists\\nDask DataFrame: Parallel Pandas DataFrames\\nMachine Learning: Parallel scikit-learn\\nOthers from external projects, like Xarray\\n\\n\\n=== Low-level ===\\nDelayed: Parallel function evaluation\\nFutures: Real-time parallel function evaluation\\nUnder the hood, each of these user interfaces adopts the same parallel computing machinery.\\n\\n\\n== High-level collections ==\\nDask\\'s high-level collections are the natural entry point for users who are interested in scaling up their pandas, NumPy or scikit-learn workload. Dask’s DataFrame, Array and Dask-ML are alternatives to Pandas DataFrame, Numpy Array and scikit-learn respectively with slight variations to the original interfaces.\\n\\n\\n=== Dask Array ===\\nDask Array is a high-level collection that parallelizes array-based workloads and maintains the familiar NumPy API, such as slicing, arithmetic, reductions, mathematics, etc., making it easy for Numpy users to scale up array operations.\\nA Dask array comprises many smaller n-dimensional Numpy arrays and uses a blocked algorithm to enable computation on larger-than-memory arrays. During an operation, Dask translates the array operation into a task graph, breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk in parallel. Results from each chunk are combined to produce the final output.\\n\\n\\n=== Dask DataFrame ===\\nDask DataFrame is a high-level collection that parallelizes DataFrame based workloads. A Dask DataFrame comprises many smaller Pandas DataFrames partitioned along the index. It maintains the familiar Pandas API, making it easy for Pandas users to scale up DataFrame workloads. During a DataFrame operation, Dask creates a task graph and triggers operations on the constituent DataFrames in a manner that reduces memory footprint and increases parallelism through sharing and deleting of intermediate results.\\n\\n\\n=== Dask Bag ===\\nDask Bag is an unordered collection of repeated objects, a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format.\\n\\n\\n== Low-level collections ==\\nThe Dask low-level interface allows for more customization. It is suitable for data that does not fall within the scope of a Dask DataFrame, Bag or Array. Dask has the following low-level collections:\\n\\nDelayed: Parallel function evaluation\\nF\\n</Document>'], 'analyst': Analyst(affiliation='Leading Data Science Consultancy', name='Dr. Amelia Wang', role='Senior Data Scientist', description=\"Dr. Amelia Wang is a senior data scientist with over a decade of experience in data analysis and machine learning. She deeply values Python for its rich set of libraries like Pandas and Scikit-learn that streamline the data cleaning and modeling phases. Amelia often cites Python's readability and simplicity as vital characteristics that enhance collaboration within her team, enabling clear communication of methodologies and results. Her motivation to advocate for Python comes from its ability to seamlessly integrate data analysis with machine learning workflows, providing a cohesive environment that fosters innovation.\"), 'transcript': \"AI: Certainly! Let's begin the interview with the expert by focusing on understanding specific aspects of integrating machine learning workflows using Python.\\n\\n**Question 1:**  \\nIn your experience, what are the most significant advantages of using Python's libraries, like Pandas and Scikit-learn, for integrating data cleaning with machine learning workflows? Specifically, how do these libraries enhance efficiency in the transition from raw data analysis to model development?\\nAI: In my experience, the significant advantages of using Python's libraries like Pandas and Scikit-learn for integrating data cleaning with machine learning workflows include their ability to streamline the entire process. Pandas excels in data manipulation, allowing users to handle and preprocess data efficiently. Scikit-learn provides a comprehensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nTogether, these libraries enhance efficiency by offering a cohesive environment where each step, from data analysis to model development, can be performed in a streamlined manner. For instance, Scikit-learn's Pipeline object allows you to easily concatenate preprocessing steps with your final machine learning estimator, ensuring that the entire workflow is manageable and reproducible [1]. This integration leverages the strengths of each library, facilitating a smooth transition from raw data to final model development [2].\\n\\nSources:  \\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/\\nAI: Thank you for that detailed explanation. \\n\\n**Question 2:**  \\nCan you elaborate on the role of Scikit-learn's Pipelines in ensuring reproducibility and consistency in machine learning projects? Specifically, how do Pipelines contribute to minimizing errors related to data processing and model training?\\nAI: Scikit-learn's Pipelines play a crucial role in ensuring reproducibility and consistency in machine learning projects by encapsulating the sequence of processing steps in machine learning tasks. This includes data preprocessing and model fitting into a single, streamlined unit [1][2]. By using Pipelines, you automatically apply all the specified preprocessing steps to new data before making predictions, which minimizes the chances of errors that often arise from inconsistent data handling or manual step execution [2].\\n\\nPipelines also reduce code complexity and help in maintaining organized workflows by chaining together sequences of data transformations and model training into a declarative structure. This structure ensures that data transformations are applied in the same order every time, thereby making the process reproducible and reducing the likelihood of human error [2][3].\\n\\nAdditionally, Pipelines aid in hyperparameter tuning as they allow for easy experimentation of different preprocessing and model training configurations simultaneously, ensuring that the best combination is found efficiently without manual intervention [2].\\n\\nSources:  \\n[1] https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4  \\n[2] https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines  \\n[3] https://www.labellerr.com/blog/end-to-end-ml-pipeline/\\nAI: Thank you so much for your help!\\nAI: You're welcome! If you have any more questions, feel free to ask. Have a great day!\", 'report': [\"## Leveraging Python Libraries for Machine Learning Workflows\\n\\n### Summary\\n\\nIntegrating machine learning (ML) workflows using Python offers substantial efficiencies, particularly through the use of libraries such as Pandas and Scikit-learn. These libraries streamline the transition from data analysis to model development by providing a cohesive environment for data manipulation and machine learning tasks.\\n\\nKey advantages include:\\n\\n- **Data Manipulation and Cleaning**: Pandas enables efficient handling and preprocessing of data, which is crucial for cleaning and transforming raw datasets for ML applications [1].\\n- **Seamless Development Environment**: Scikit-learn complements Pandas by offering an extensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\\n\\nThe role of Scikit-learn's Pipelines is particularly noteworthy:\\n\\n- **Reproducibility and Consistency**: Pipelines encapsulate processing steps, ensuring data preprocessing and model fitting are applied consistently. This reduces errors because each step is automatically repeated with new data, enhancing overall reproducibility [3][4].\\n- **Simplified Workflow Management**: Pipelines reduce code complexity by chaining together sequences of data transformations and model training into a single, manageable unit. This modular approach assists users in maintaining organized workflows [4][5].\\n- **Efficient Parameter Tuning**: Through Pipelines, users can conduct hyperparameter tuning more effectively, as every combination of preprocessing and model configurations can be explored without manual intervention [4].\\n\\nScikit-learn is widely leveraged across various industries, including finance and retail, to handle tasks like fraud detection and predictive analytics, demonstrating its practical utility in real-world applications [6].\\n\\nOverall, Python's machine learning libraries enhance workflow efficiency by supporting seamless integration of data manipulation, model development, and process optimization.\\n\\n### Sources\\n[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \\n[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/  \\n[3] https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4  \\n[4] https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines  \\n[5] https://www.labellerr.com/blog/end-to-end-ml-pipeline/  \\n[6] https://en.wikipedia.org/wiki/Scikit-learn  \"]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": 323456}}\n",
    "test_state = InterviewState(\n",
    "    analyst=analysts[1],\n",
    "    max_num_turns=3,\n",
    "    messages=[]\n",
    ")\n",
    "\n",
    "for event in graph.stream(test_state, thread, stream_mode=\"values\"):\n",
    "    print(event, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ba3192a-00c9-477e-af11-a68b1f1f5443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Leveraging Python Libraries for Machine Learning Workflows\n",
       "\n",
       "### Summary\n",
       "\n",
       "Integrating machine learning (ML) workflows using Python offers substantial efficiencies, particularly through the use of libraries such as Pandas and Scikit-learn. These libraries streamline the transition from data analysis to model development by providing a cohesive environment for data manipulation and machine learning tasks.\n",
       "\n",
       "Key advantages include:\n",
       "\n",
       "- **Data Manipulation and Cleaning**: Pandas enables efficient handling and preprocessing of data, which is crucial for cleaning and transforming raw datasets for ML applications [1].\n",
       "- **Seamless Development Environment**: Scikit-learn complements Pandas by offering an extensive suite of machine learning algorithms and preprocessing tools that are designed to work seamlessly with other scientific libraries such as NumPy and SciPy [1][2].\n",
       "\n",
       "The role of Scikit-learn's Pipelines is particularly noteworthy:\n",
       "\n",
       "- **Reproducibility and Consistency**: Pipelines encapsulate processing steps, ensuring data preprocessing and model fitting are applied consistently. This reduces errors because each step is automatically repeated with new data, enhancing overall reproducibility [3][4].\n",
       "- **Simplified Workflow Management**: Pipelines reduce code complexity by chaining together sequences of data transformations and model training into a single, manageable unit. This modular approach assists users in maintaining organized workflows [4][5].\n",
       "- **Efficient Parameter Tuning**: Through Pipelines, users can conduct hyperparameter tuning more effectively, as every combination of preprocessing and model configurations can be explored without manual intervention [4].\n",
       "\n",
       "Scikit-learn is widely leveraged across various industries, including finance and retail, to handle tasks like fraud detection and predictive analytics, demonstrating its practical utility in real-world applications [6].\n",
       "\n",
       "Overall, Python's machine learning libraries enhance workflow efficiency by supporting seamless integration of data manipulation, model development, and process optimization.\n",
       "\n",
       "### Sources\n",
       "[1] https://stackoverflow.com/questions/66114523/should-one-use-pandas-or-sklearn-for-imputation-normalization-etc  \n",
       "[2] https://machinelearningmastery.com/how-to-combine-pandas-numpy-and-scikit-learn-seamlessly/  \n",
       "[3] https://medium.com/@sahin.samia/scikit-learn-pipelines-explained-streamline-and-optimize-your-machine-learning-processes-f17b1beb86a4  \n",
       "[4] https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines  \n",
       "[5] https://www.labellerr.com/blog/end-to-end-ml-pipeline/  \n",
       "[6] https://en.wikipedia.org/wiki/Scikit-learn  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "state = graph.get_state(thread)\n",
    "Markdown(state.values[\"report\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e8d68-3f3c-437f-949a-a68a0bbfe873",
   "metadata": {},
   "source": [
    "---\n",
    "# Finilize the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6ed71eb-ae35-4cbd-86a3-f93ae4be8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchGraphState(TypedDict):\n",
    "    topic: str  # Research topic\n",
    "    max_analysts: int  # Number of analysts\n",
    "    analysts = List[Analyst]  # Analysts persona\n",
    "    human_feedback: str  # Human feedback for analysts\n",
    "    sections: Annotated[list, operator.add]  # conversations report\n",
    "    introduction: str  # Introduction for the final report\n",
    "    content: str  # Content for the final report\n",
    "    conclusion: str  # Conclusion for the final report\n",
    "    final_report: str  # Final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f177389-2ed8-4772-91c9-326d2515bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New router node for initiating interviews in parallel\n",
    "def initiate_interviews(state: ResearchGraphState):\n",
    "    \"\"\" This is the \"map\" step where we run each interview sub-graph using Send API \"\"\"    \n",
    "    human_feedback = state.get(\"human_feedback\", None)\n",
    "\n",
    "    # Check if human feedback is provided\n",
    "    if human_feedback:\n",
    "        # return to create_analyst node\n",
    "        return \"create_analysts\"\n",
    "    \n",
    "    # Start interviews in parallel using Send() API\n",
    "    topic = state[\"topic\"]\n",
    "    return [Send(\n",
    "        \"conduct_interview\", \n",
    "        {\n",
    "            \"analyst\": analyst\n",
    "        }\n",
    "    ) for analyst in state[\"analysts\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26247377-5720-48c4-9d56-6dfd5f6971bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
    "\n",
    "You will be given all of the sections of the report.\n",
    "\n",
    "You job is to write a crisp and compelling introduction or conclusion section.\n",
    "\n",
    "The user will instruct you whether to write the introduction or conclusion.\n",
    "\n",
    "Include no pre-amble for either section.\n",
    "\n",
    "Target around 100 words, crisply previewing (for introduction) or recapping (for conclusion) all of the sections of the report.\n",
    "\n",
    "Use markdown formatting. \n",
    "\n",
    "For your introduction, create a compelling title and use the # header for the title.\n",
    "\n",
    "For your introduction, use ## Introduction as the section header. \n",
    "\n",
    "For your conclusion, use ## Conclusion as the section header.\n",
    "\n",
    "Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n",
    "\n",
    "\n",
    "report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic: \n",
    "\n",
    "{topic}\n",
    "    \n",
    "You have a team of analysts. Each analyst has done two things: \n",
    "\n",
    "1. They conducted an interview with an expert on a specific sub-topic.\n",
    "2. They write up their finding into a memo.\n",
    "\n",
    "Your task: \n",
    "\n",
    "1. You will be given a collection of memos from your analysts.\n",
    "2. Think carefully about the insights from each memo.\n",
    "3. Consolidate these into a crisp overall summary that ties together the central ideas from all of the memos. \n",
    "4. Summarize the central points in each memo into a cohesive single narrative.\n",
    "\n",
    "To format your report:\n",
    " \n",
    "1. Use markdown formatting. \n",
    "2. Include no pre-amble for the report.\n",
    "3. Use no sub-heading. \n",
    "4. Start your report with a single title header: ## Insights\n",
    "5. Do not mention any analyst names in your report.\n",
    "6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2].\n",
    "7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header.\n",
    "8. List your sources in order and do not repeat.\n",
    "\n",
    "[1] Source 1\n",
    "[2] Source 2\n",
    "\n",
    "Here are the memos from your analysts to build your report from: \n",
    "\n",
    "{context}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afc353c2-dacc-4027-9031-eef6122a2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def write_report(state: ResearchGraphState):\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    system_message = report_writer_instructions.format(topic=topic, context=formatted_str_sections)    \n",
    "    report = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f\"Write a report based upon these memos.\")]) \n",
    "    return {\"content\": report.content}\n",
    "\n",
    "\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    # Full set of sections\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    \n",
    "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)    \n",
    "    intro = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report introduction\")]) \n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    # Full set of sections\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    \n",
    "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)    \n",
    "    conclusion = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report conclusion\")]) \n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    \"\"\" The is the \"reduce\" step where we gather all the sections, combine them, and reflect on them to write the intro/conclusion \"\"\"\n",
    "    # Save full final report\n",
    "    content = state[\"content\"]\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    final_report = state[\"introduction\"] + \"\\n\\n---\\n\\n\" + content + \"\\n\\n---\\n\\n\" + state[\"conclusion\"]\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "    return {\"final_report\": final_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b708aaed-3603-43a9-9b81-5032aaded50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
    "builder.add_node(\"write_introduction\",write_introduction)\n",
    "builder.add_node(\"write_report\",write_report)\n",
    "builder.add_node(\"write_conclusion\",write_conclusion)\n",
    "builder.add_node(\"finalize_report\",finalize_report)\n",
    "\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\", initiate_interviews, [\"create_analysts\", \"conduct_interview\"])\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "builder.add_edge([\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\")\n",
    "builder.add_edge(\"finalize_report\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eab70e1-39ba-4cdc-a340-382bb9f1a2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Carter\n",
      "Affiliation: Digital Transformation Think Tank\n",
      "Role: Technology Strategist\n",
      "Description: Dr. Emily Carter is a technology strategist specializing in emerging digital frameworks and platforms. Her primary focus is on assessing the strategic benefits and long-term impacts of adopting innovative technologies like LangGraph. She is particularly concerned with how such technologies can drive efficiency, scalability, and competitive advantage within organizations. Emily is motivated by the desire to help companies navigate the complexities of digital transformation and stay ahead in the fast-paced tech landscape.\n",
      "--------------------------------------------------\n",
      "Name: Rajesh Iyer\n",
      "Affiliation: Global Tech Consultancy Firm\n",
      "Role: Software Development Expert\n",
      "Description: Rajesh Iyer is a seasoned software development expert with extensive experience in implementing advanced programming frameworks. He is focused on evaluating the technical benefits and potential challenges of adopting LangGraph from a developer's perspective. Rajesh is particularly interested in the framework's capabilities in simplifying code structure, enhancing developer productivity, and integrating with existing systems. His motivation lies in optimizing development processes to improve software quality and reduce time-to-market for tech solutions.\n",
      "--------------------------------------------------\n",
      "Name: Laura Mendes\n",
      "Affiliation: Institute for Economic and Business Research\n",
      "Role: Business Analyst\n",
      "Description: Laura Mendes is a business analyst with a focus on the economic implications of technology adoption. She explores the cost-benefit analysis of integrating LangGraph into business operations. Her concerns include the initial investment, potential return on investment, and the broader economic impacts such as job market changes. Laura's motivation is driven by helping businesses make informed decisions that align technological advancements with economic growth and sustainability.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "max_analysts = 3 \n",
    "topic = \"The benefits of adopting LangGraph as an agent framework\"\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream({\"topic\":topic,\n",
    "                           \"max_analysts\":max_analysts}, \n",
    "                          thread, \n",
    "                          stream_mode=\"values\"):\n",
    "    \n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "349035d6-14eb-4d5c-a605-196ac24b2081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f093086-db4b-63b0-8002-cbf85f45f3f6'}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now update the state as if we are the human_feedback node\n",
    "graph.update_state(thread, {\"human_feedback\": \n",
    "                                \"Add in the CEO of gen ai native startup\"}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bfc88e6-1e51-4631-b8d5-00539bd1576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Carter\n",
      "Affiliation: Digital Transformation Think Tank\n",
      "Role: Technology Strategist\n",
      "Description: Dr. Emily Carter is a technology strategist specializing in emerging digital frameworks and platforms. Her primary focus is on assessing the strategic benefits and long-term impacts of adopting innovative technologies like LangGraph. She is particularly concerned with how such technologies can drive efficiency, scalability, and competitive advantage within organizations. Emily is motivated by the desire to help companies navigate the complexities of digital transformation and stay ahead in the fast-paced tech landscape.\n",
      "--------------------------------------------------\n",
      "Name: Rajesh Iyer\n",
      "Affiliation: Global Tech Consultancy Firm\n",
      "Role: Software Development Expert\n",
      "Description: Rajesh Iyer is a seasoned software development expert with extensive experience in implementing advanced programming frameworks. He is focused on evaluating the technical benefits and potential challenges of adopting LangGraph from a developer's perspective. Rajesh is particularly interested in the framework's capabilities in simplifying code structure, enhancing developer productivity, and integrating with existing systems. His motivation lies in optimizing development processes to improve software quality and reduce time-to-market for tech solutions.\n",
      "--------------------------------------------------\n",
      "Name: Laura Mendes\n",
      "Affiliation: Institute for Economic and Business Research\n",
      "Role: Business Analyst\n",
      "Description: Laura Mendes is a business analyst with a focus on the economic implications of technology adoption. She explores the cost-benefit analysis of integrating LangGraph into business operations. Her concerns include the initial investment, potential return on investment, and the broader economic impacts such as job market changes. Laura's motivation is driven by helping businesses make informed decisions that align technological advancements with economic growth and sustainability.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Alison Chang\n",
      "Affiliation: LangGraph\n",
      "Role: Chief Developer\n",
      "Description: Dr. Alison Chang is a passionate software developer who leads the team behind LangGraph. She is deeply focused on advancing agent frameworks and strives to demonstrate the technical benefits of using LangGraph, such as ease of integration, scalability, and real-time processing capabilities. Her motivation is to push technological boundaries and make LangGraph a leading platform for building intelligent systems.\n",
      "--------------------------------------------------\n",
      "Name: Carlos Moreno\n",
      "Affiliation: Gen AI Native Startup\n",
      "Role: CEO\n",
      "Description: Carlos Moreno is the CEO of a leading Gen AI native startup that is always on the lookout for cutting-edge technologies to gain a competitive advantage. He is motivated by the potential business benefits that LangGraph offers, such as cost savings, increased innovation potential, and better market positioning. Carlos is particularly interested in understanding how adopting LangGraph can streamline operations and enhance product offerings.\n",
      "--------------------------------------------------\n",
      "Name: Emily Rand\n",
      "Affiliation: Independent Technology Analyst\n",
      "Role: Industry Analyst\n",
      "Description: Emily Rand is an experienced independent technology analyst who specializes in evaluating emerging technologies for their impact on business strategies. She investigates LangGraph from various perspectives, including its technological maturity, entry barriers, and potential for industry disruption. Her analytical reports cater to both technical and business audiences, providing unbiased insights into the advantages and challenges of adopting new frameworks like LangGraph.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78064328-77ae-4cdc-980f-406337182e8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'analysts'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Confirm we are happy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman_feedback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_node\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman_feedback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/llm-engineering-12-week-roadmap/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2328\u001b[39m, in \u001b[36mPregel.update_state\u001b[39m\u001b[34m(self, config, values, as_node, task_id)\u001b[39m\n\u001b[32m   2317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_state\u001b[39m(\n\u001b[32m   2318\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2319\u001b[39m     config: RunnableConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2322\u001b[39m     task_id: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2323\u001b[39m ) -> RunnableConfig:\n\u001b[32m   2324\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Update the state of the graph with the given values, as if they came from\u001b[39;00m\n\u001b[32m   2325\u001b[39m \u001b[33;03m    node `as_node`. If `as_node` is not provided, it will be set to the last node\u001b[39;00m\n\u001b[32m   2326\u001b[39m \u001b[33;03m    that updated the state, if not ambiguous.\u001b[39;00m\n\u001b[32m   2327\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbulk_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStateUpdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/llm-engineering-12-week-roadmap/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:1861\u001b[39m, in \u001b[36mPregel.bulk_update_state\u001b[39m\u001b[34m(self, config, supersteps)\u001b[39m\n\u001b[32m   1857\u001b[39m current_config = patch_configurable(\n\u001b[32m   1858\u001b[39m     config, {CONFIG_KEY_THREAD_ID: \u001b[38;5;28mstr\u001b[39m(config[CONF][CONFIG_KEY_THREAD_ID])}\n\u001b[32m   1859\u001b[39m )\n\u001b[32m   1860\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m superstep \u001b[38;5;129;01min\u001b[39;00m supersteps:\n\u001b[32m-> \u001b[39m\u001b[32m1861\u001b[39m     current_config = \u001b[43mperform_superstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuperstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m current_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/llm-engineering-12-week-roadmap/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:1796\u001b[39m, in \u001b[36mPregel.bulk_update_state.<locals>.perform_superstep\u001b[39m\u001b[34m(input_config, updates)\u001b[39m\n\u001b[32m   1794\u001b[39m     run = RunnableSequence(*writers) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(writers) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m writers[\u001b[32m0\u001b[39m]\n\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# execute task\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1796\u001b[39m     \u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1799\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1800\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUpdateState\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# deque.extend is thread-safe\u001b[39;49;00m\n\u001b[32m   1803\u001b[39m \u001b[43m                \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1804\u001b[39m \u001b[43m                \u001b[49m\u001b[43mCONFIG_KEY_TASK_ID\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1805\u001b[39m \u001b[43m                \u001b[49m\u001b[43mCONFIG_KEY_READ\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mlocal_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m_scratchpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[43m                        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmanaged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1819\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[38;5;66;03m# save task writes\u001b[39;00m\n\u001b[32m   1824\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task_id, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_task_ids, run_tasks):\n\u001b[32m   1825\u001b[39m     \u001b[38;5;66;03m# channel writes are saved to current checkpoint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/llm-engineering-12-week-roadmap/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3082\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3080\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3081\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3082\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/llm-engineering-12-week-roadmap/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/llm-engineering-12-week-roadmap/.venv/lib/python3.12/site-packages/langgraph/graph/_branch.py:168\u001b[39m, in \u001b[36mBranchSpec._route\u001b[39m\u001b[34m(self, input, config, reader, writer)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    167\u001b[39m     value = \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._finish(writer, \u001b[38;5;28minput\u001b[39m, result, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/llm-engineering-12-week-roadmap/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:394\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    396\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36minitiate_interviews\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Start interviews in parallel using Send() API\u001b[39;00m\n\u001b[32m     12\u001b[39m topic = state[\u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [Send(\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconduct_interview\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     15\u001b[39m     {\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33manalyst\u001b[39m\u001b[33m\"\u001b[39m: analyst\n\u001b[32m     17\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m ) \u001b[38;5;28;01mfor\u001b[39;00m analyst \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manalysts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m]\n",
      "\u001b[31mKeyError\u001b[39m: 'analysts'"
     ]
    }
   ],
   "source": [
    "# Confirm we are happy\n",
    "graph.update_state(thread, {\"human_feedback\": \n",
    "                            None}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "320da8c0-986c-4a35-b7cb-bdb1b93a8529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Node--\n",
      "create_analysts\n",
      "--Node--\n",
      "__interrupt__\n"
     ]
    }
   ],
   "source": [
    "# Continue\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b80acb0f-4ab4-4f37-b704-cfefcf8c84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbeb86d3-e223-40ce-aa8b-4610869cf99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237833d8-aa15-4735-b330-2869ce54d66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

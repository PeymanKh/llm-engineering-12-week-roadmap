{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Document Loaders\n",
    "\n",
    "Document loaders are designed to load document objects. Here is a list of different document loaders in LangChain.\n",
    "\n",
    "\n",
    "### 1. PDF Loaders"
   ],
   "id": "f5c9512f20a73b5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:01:50.540822Z",
     "start_time": "2025-09-26T16:01:50.284041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"myPDF.pdf\")\n",
    "\n",
    "document = loader.load()\n",
    "pages = [page for page in document]\n",
    "\n",
    "pages[3]"
   ],
   "id": "b7709038761e94eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'author': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'myPDF.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='4 Z. Shen et al.\\nEfficient Data Annotation\\nC u s t o m i z e d  M o d e l  T r a i n i n g\\nModel Cust omization\\nDI A Model Hub\\nDI A Pipeline Sharing\\nCommunity Platform\\nLa y out Detection Models\\nDocument Images \\nT h e  C o r e  L a y o u t P a r s e r  L i b r a r y\\nOCR Module St or age & VisualizationLa y out Data Structur e\\nFig. 1: The overall architecture of LayoutParser. For an input document image,\\nthe core LayoutParser library provides a set of oﬀ-the-shelf tools for layout\\ndetection, OCR, visualization, and storage, backed by a carefully designed layout\\ndata structure. LayoutParser also supports high level customization via eﬃcient\\nlayout annotation and model training functions. These improve model accuracy\\non the target samples. The community platform enables the easy sharing of DIA\\nmodels and whole digitization pipelines to promote reusability and reproducibility.\\nA collection of detailed documentation, tutorials and exemplar projects make\\nLayoutParser easy to learn and use.\\nAllenNLP [8] and transformers [ 34] have provided the community with complete\\nDL-based support for developing and deploying models for general computer\\nvision and natural language processing problems. LayoutParser, on the other\\nhand, specializes speciﬁcally in DIA tasks. LayoutParser is also equipped with a\\ncommunity platform inspired by established model hubs such as Torch Hub [23]\\nand TensorFlow Hub [1]. It enables the sharing of pretrained models as well as\\nfull document processing pipelines that are unique to DIA tasks.\\nThere have been a variety of document data collections to facilitate the\\ndevelopment of DL models. Some examples include PRImA [3](magazine layouts),\\nPubLayNet [38](academic paper layouts), Table Bank [ 18](tables in academic\\npapers), Newspaper Navigator Dataset [ 16, 17](newspaper ﬁgure layouts) and\\nHJDataset [31](historical Japanese document layouts). A spectrum of models\\ntrained on these datasets are currently available in the LayoutParser model zoo\\nto support diﬀerent use cases.\\n3 The Core LayoutParser Library\\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL-\\nbased document image analysis. Five components support a simple interface\\nwith comprehensive functionalities: 1) The layout detection models enable using\\npre-trained or self-trained DL models for layout detection with just four lines\\nof code. 2) The detected layout information is stored in carefully engineered')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Limitations of PyPDFLoader:\n",
    "\n",
    "1. **No OCR:** Can't extract images, and text from images or handwritten.\n",
    "2. **No Layout Analysis:** Can't distinguish between headers, paragraphs, or tables."
   ],
   "id": "76a5a469c95fd528"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:01:34.980247Z",
     "start_time": "2025-09-26T16:01:33.569420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\n",
    "    \"myPDF.pdf\",\n",
    "    mode=\"page\",\n",
    "    extract_images=True,\n",
    "    extract_tables=\"markdown\"\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "pages = [page for page in document]\n",
    "\n",
    "pages[3]"
   ],
   "id": "1ef169bebe0e1b0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'author': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'myPDF.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='4 Z. Shen et al.\\nEfficient Data Annotation\\nC u s t o m i z e d  M o d e l  T r a i n i n g\\nModel Cust omization\\nDI A Model Hub\\nDI A Pipeline Sharing\\nCommunity Platform\\nLa y out Detection Models\\nDocument Images \\nT h e  C o r e  L a y o u t P a r s e r  L i b r a r y\\nOCR Module St or age & VisualizationLa y out Data Structur e\\nFig. 1: The overall architecture of LayoutParser. For an input document image,\\nthe core LayoutParser library provides a set of oﬀ-the-shelf tools for layout\\ndetection, OCR, visualization, and storage, backed by a carefully designed layout\\ndata structure. LayoutParser also supports high level customization via eﬃcient\\nlayout annotation and model training functions. These improve model accuracy\\non the target samples. The community platform enables the easy sharing of DIA\\nmodels and whole digitization pipelines to promote reusability and reproducibility.\\nA collection of detailed documentation, tutorials and exemplar projects make\\nLayoutParser easy to learn and use.\\nAllenNLP [8] and transformers [ 34] have provided the community with complete\\nDL-based support for developing and deploying models for general computer\\nvision and natural language processing problems. LayoutParser, on the other\\nhand, specializes speciﬁcally in DIA tasks. LayoutParser is also equipped with a\\ncommunity platform inspired by established model hubs such as Torch Hub [23]\\nand TensorFlow Hub [1]. It enables the sharing of pretrained models as well as\\nfull document processing pipelines that are unique to DIA tasks.\\nThere have been a variety of document data collections to facilitate the\\ndevelopment of DL models. Some examples include PRImA [3](magazine layouts),\\nPubLayNet [38](academic paper layouts), Table Bank [ 18](tables in academic\\npapers), Newspaper Navigator Dataset [ 16, 17](newspaper ﬁgure layouts) and\\nHJDataset [31](historical Japanese document layouts). A spectrum of models\\ntrained on these datasets are currently available in the LayoutParser model zoo\\nto support diﬀerent use cases.\\n3 The Core LayoutParser Library\\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL-\\nbased document image analysis. Five components support a simple interface\\nwith comprehensive functionalities: 1) The layout detection models enable using\\npre-trained or self-trained DL models for layout detection with just four lines\\nof code. 2) The detected layout information is stored in carefully engineered')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 2. Webpage Loaders\n"
   ],
   "id": "dda34d3ada68f8b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:23:42.159057Z",
     "start_time": "2025-09-26T16:23:41.431046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://peymankh.dev\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "docs"
   ],
   "id": "c92f0223753255f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://peymankh.dev', 'title': 'Peyman KH | AI Engineer & Developer', 'description': 'Peyman Khodabandehlouei, an AI Engineer & developer specializing in AI orchestration, multi-agent systems, and production-grade automation', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nPeyman KH | AI Engineer & Developer\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:35:11.676111Z",
     "start_time": "2025-09-26T16:34:11.247065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "\n",
    "loader = FireCrawlLoader(\n",
    "    api_key=\"fc-3f2e8f2c31ca4f23a9ec5e1c717ceef9\",\n",
    "    url=\"https://binance.com\",\n",
    "    mode=\"scrape\",\n",
    "    api_url=\"https://api.firecrawl.dev\"  # Add this parameter explicitly\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ],
   "id": "7030d7ca7d983e",
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "Request Timeout: Failed to scrape URL as the request timed out. Scrape timed out after waiting in the concurrency limit queue",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_community\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdocument_loaders\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FireCrawlLoader\n\u001B[32m      3\u001B[39m loader = FireCrawlLoader(\n\u001B[32m      4\u001B[39m     api_key=\u001B[33m\"\u001B[39m\u001B[33mfc-3f2e8f2c31ca4f23a9ec5e1c717ceef9\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      5\u001B[39m     url=\u001B[33m\"\u001B[39m\u001B[33mhttps://binance.com\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      6\u001B[39m     mode=\u001B[33m\"\u001B[39m\u001B[33mscrape\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      7\u001B[39m     api_url=\u001B[33m\"\u001B[39m\u001B[33mhttps://api.firecrawl.dev\u001B[39m\u001B[33m\"\u001B[39m  \u001B[38;5;66;03m# Add this parameter explicitly\u001B[39;00m\n\u001B[32m      8\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m docs = \u001B[43mloader\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/github/llm-engineering-12-week-roadmap/.venv/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:32\u001B[39m, in \u001B[36mBaseLoader.load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28mlist\u001B[39m[Document]:\n\u001B[32m     31\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Load data into Document objects.\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlazy_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/github/llm-engineering-12-week-roadmap/.venv/lib/python3.12/site-packages/langchain_community/document_loaders/firecrawl.py:276\u001B[39m, in \u001B[36mFireCrawlLoader.lazy_load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    273\u001B[39m \u001B[38;5;28mself\u001B[39m.params[\u001B[33m\"\u001B[39m\u001B[33mintegration\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33mlangchain\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    274\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.mode == \u001B[33m\"\u001B[39m\u001B[33mscrape\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    275\u001B[39m     firecrawl_docs = [\n\u001B[32m--> \u001B[39m\u001B[32m276\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfirecrawl\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscrape_url\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    277\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlegacy_scrape_options_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    278\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    279\u001B[39m     ]\n\u001B[32m    280\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.mode == \u001B[33m\"\u001B[39m\u001B[33mcrawl\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    281\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.url:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/github/llm-engineering-12-week-roadmap/.venv/lib/python3.12/site-packages/firecrawl/firecrawl.py:91\u001B[39m, in \u001B[36mFirecrawlApp.scrape_url\u001B[39m\u001B[34m(self, url, params)\u001B[39m\n\u001B[32m     89\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mFailed to scrape URL. Error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse[\u001B[33m\"\u001B[39m\u001B[33merror\u001B[39m\u001B[33m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_handle_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mscrape URL\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/github/llm-engineering-12-week-roadmap/.venv/lib/python3.12/site-packages/firecrawl/firecrawl.py:317\u001B[39m, in \u001B[36mFirecrawlApp._handle_error\u001B[39m\u001B[34m(self, response, action)\u001B[39m\n\u001B[32m    314\u001B[39m     message = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnexpected error during \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maction\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: Status code \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse.status_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[38;5;132;01m{\u001B[39;00merror_message\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    316\u001B[39m \u001B[38;5;66;03m# Raise an HTTPError with the custom message and attach the response\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m317\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m requests.exceptions.HTTPError(message, response=response)\n",
      "\u001B[31mHTTPError\u001B[39m: Request Timeout: Failed to scrape URL as the request timed out. Scrape timed out after waiting in the concurrency limit queue"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:34:06.465857Z",
     "start_time": "2025-09-26T16:34:06.453125Z"
    }
   },
   "cell_type": "code",
   "source": "docs",
   "id": "3efeb7e4f76ce541",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'og:image': '/hero-headshot.jpg', 'og:title': 'Peyman KH | AI Engineer & Developer', 'twitter:description': 'AI Engineering Student & Full-Stack Developer specializing in AI orchestration, multi-agent systems, and production-grade automation.', 'twitter:card': 'summary_large_image', 'viewport': 'width=device-width, initial-scale=1.0', 'twitter:image': '/hero-headshot.jpg', 'og:type': 'website', 'ogImage': '/hero-headshot.jpg', 'title': 'Peyman KH | AI Engineer & Developer', 'og:description': 'AI Engineering Student & Full-Stack Developer specializing in AI orchestration, multi-agent systems, and production-grade automation. Graduating 2026.', 'language': 'en', 'twitter:title': 'Peyman KH | AI Engineer & Developer', 'ogDescription': 'AI Engineering Student & Full-Stack Developer specializing in AI orchestration, multi-agent systems, and production-grade automation. Graduating 2026.', 'author': 'AI Engineering Portfolio', 'ogTitle': 'Peyman KH | AI Engineer & Developer', 'description': 'Peyman Khodabandehlouei, an AI Engineer & developer specializing in AI orchestration, multi-agent systems, and production-grade automation', 'favicon': 'https://peymankh.dev/favicon.png', 'scrapeId': '04b0b547-51ac-475d-9ecb-19fdee6f2943', 'sourceURL': 'https://peymankh.dev', 'url': 'https://peymankh.dev/', 'contentType': 'text/html; charset=utf-8', 'proxyUsed': 'basic', 'cacheState': 'miss', 'creditsUsed': 1, 'pageStatusCode': 200}, page_content=\"Peyman KH\\n\\n[About](https://peymankh.dev/#about) [Experience](https://peymankh.dev/#experience) [Open Source](https://peymankh.dev/#opensource) [Projects](https://peymankh.dev/#projects) [Certificates](https://peymankh.dev/#certificates) [Contact](https://peymankh.dev/#contact)\\n\\n[GitHub](https://github.com/PeymanKh)[LinkedIn](https://www.linkedin.com/in/peyman-khodabandehlouei)[Email](mailto:peymankhodabandehlouei@gmail.com)\\n\\n[About](https://peymankh.dev/#about) [Experience](https://peymankh.dev/#experience) [Open Source](https://peymankh.dev/#opensource) [Projects](https://peymankh.dev/#projects) [Certificates](https://peymankh.dev/#certificates) [Contact](https://peymankh.dev/#contact)\\n\\n[GitHub](https://github.com/PeymanKh)[LinkedIn](https://www.linkedin.com/in/peyman-khodabandehlouei)[Email](mailto:peymankhodabandehlouei@gmail.com)\\n\\n![AI Engineering Student](https://peymankh.dev/assets/hero-headshot-BSyj3hzD.jpg)\\n\\n# Hello, I'm Peyman\\n\\nAI Engineer & Developer\\\\|\\n\\nSpecializing in AI orchestration, multi-agent systems, and production-grade automation\\n\\nView My ProjectsDownload Resume\\n\\nScroll to explore\\n\\n## About Me\\n\\nSince childhood, curiosity has been an inseparable part of me. Sometimes it leads me into challenging situations, other times it sparks moments of brilliance. In 2021, it pushed me to leave my parents home to pursue AI engineering at Bahçeşehir University in Istanbul. I'm the kind of person who values one deep conversation over ten small talks, which is exactly why AI captivated me so deeply that I've been all in since my first semester. Going all in means full commitment, meeting incredible people in the field, launching projects in production environment, and sharing my projects openly because I believe knowledge should be shared.\\n\\nExplore My Technical Journey\\n\\n## Experience\\n\\n### Data Engineering Intern\\n\\nDestade\\n\\nJul 2025 - Sep 2025\\n\\nIstanbul, Turkey\\n\\nRebuilding hospital data architecture from the ground up. Designing database schemas, implementing data pipelines, and creating AI-powered analytics tools for healthcare management systems.\\n\\n#### Key Achievements\\n\\n- Redesigned and implemented optimized database schema to resolve complex NoSQL data architecture issues\\n- Executed seamless data migration process, ensuring zero data loss during database restructuring\\n- Developed comprehensive management dashboards for hospital operations monitoring and decision-making\\n- Designed and implemented RESTful API endpoints to efficiently serve hospital data to UI layer\\n- Built AI-powered chart analysis assistant capable of interpreting and analyzing medical chart data\\n\\n#### Technologies\\n\\nPython\\n\\nFlutter\\n\\nLangChain\\n\\nLangGraph\\n\\nMongoDB\\n\\nFirebase\\n\\nFastAPI\\n\\nGCP\\n\\n### Student Researcher\\n\\nUniversity Blockchain Research Center\\n\\nMar 2024 - Jun 2025\\n\\nIstanbul, Turkey\\n\\nContributed to blockchain research projects focusing on cryptocurrency data analysis and backend development. Built comprehensive data pipelines and analytical tools for blockchain research initiatives.\\n\\n#### Key Achievements\\n\\n- Designed and managed both SQL and NoSQL database architectures for on-chain data storage\\n- Implemented interactive dashboards to visualize complex analytics and present data insights\\n- Developed end-to-end data pipelines for cryptocurrency on-chain data collection and processing\\n- Implemented sentiment analysis systems for cryptocurrency news monitoring\\n- Built RESTful API endpoints serving blockchain analytics to research applications\\n\\n#### Technologies\\n\\nPython\\n\\nFastAPI\\n\\nLangChain\\n\\nNLTK\\n\\nPandas\\n\\nScikit-Learn\\n\\nMongoDB\\n\\nPlotly & Dash\\n\\nGCP\\n\\n## Public Activities\\n\\n### Bitcoin On-Chain Metrics API\\n\\nA comprehensive REST API designed specifically for researchers, academics, and students to access and analyze critical Bitcoin blockchain metrics. This project democratizes access to sophisticated on-chain data by providing structured endpoints for querying historical blockchain analytics that are typically available only through expensive commercial platforms.\\n\\nExplore API DocumentationView API Docs\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n1\\n\\n1\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n1\\n\\n0\\n\\n0\\n\\n0\\n\\n## Projects\\n\\n### Live Applications\\n\\n### Delisio - Your Personal Chef\\n\\nLive\\n\\nDelisio personalizes recipes for any food from its image based on your diet type, nutritional goals, allergies, and available equipment. Making healthy eating simple and personalized.\\n\\nFlutter\\n\\nPython\\n\\nLangGraph\\n\\nMicroservices\\n\\nFireBase\\n\\nAWS\\n\\n[App Store](https://apps.apple.com/us/app/delisio-your-personal-chef/id6744915908) [Research Report](https://github.com/PeymanKh/delisio_mvp/blob/main/paper/technical_report.pdf)\\n\\n### Authenaa - Telegram Community Manager\\n\\nLive\\n\\nAuthenaa is a multi-agent verification system for crypto influencers partnered with exchanges. It verifies users by exchange UID, monitors trading volume, and removes inactive users to maximize referral income.\\n\\nTelegram\\n\\nPython\\n\\nLangGraph\\n\\nMongoDB\\n\\nDocker\\n\\nGCP\\n\\n[Visit Website](https://authenaa.com/)\\n\\n### Personal Projects\\n\\n### Smart Recipe Generator\\n\\nCNN food recognition combined with LLM recipe generation using LangGraph state machine. Intelligent workflow for personalized recipe creation from food images.\\n\\nPython\\n\\nJupyter Notebook\\n\\nClassification\\n\\nTensorflow\\n\\nLangGraph\\n\\n[View Repository](https://github.com/PeymanKh/delisio_mvp)\\n\\n### News Monitoring Pipeline\\n\\nAutomated cryptocurrency news aggregation with sentiment analysis. Fetches data from API, extracts insights, and delivers findings via Telegram bot integration.\\n\\nPython\\n\\nJupyter Notebook\\n\\nCrypto\\n\\nSentiment Analysis\\n\\nLangGraph\\n\\n[View Repository](https://github.com/PeymanKh/crypto_news_pipeline)\\n\\n### Maze Navigator Agent\\n\\nAI pathfinding agent implementing DFS, BFS, and A\\\\* search algorithms with Manhattan heuristic. Features random maze generation and visualization of search strategies.\\n\\nPython\\n\\nPathfinding\\n\\nBFS\\n\\nDFS\\n\\nA\\\\*\\n\\n[View Repository](https://github.com/PeymanKh/maze_navigator_agent)\\n\\n### Weather Forecasting LSTM\\n\\nWeather prediction LSTM neural network to forecast next day's temperature. Built with 10 years of historical Istanbul weather data collected via Meteostat API.\\n\\nPython\\n\\nJupyter Notebook\\n\\nLSTM\\n\\nTime-Series\\n\\nTensorflow\\n\\n[View Repository](https://github.com/PeymanKh/weather_forecasting_lstm/tree/main)\\n\\n### Genetic Route Optimizer\\n\\nGenetic algorithm solving the Traveling Salesman Problem with evolutionary techniques. Implements crossover, mutation, and selection operators.\\n\\nPython\\n\\nTSP\\n\\nGenetic Algorithm\\n\\nGraph\\n\\n[View Repository](https://github.com/PeymanKh/solve_tsp_using_genetic_algorithm)\\n\\n### Microservices Bookstore API\\n\\nFlask REST API with MongoDB backend deployed on Kubernetes cluster. Features CRUD operations, Docker containerization, and StatefulSet database deployment.\\n\\nPython\\n\\nDockerfile\\n\\nMicroservices\\n\\nDocker\\n\\nKubernetes\\n\\n[View Repository](https://github.com/PeymanKh/microservices-bookstore-app)\\n\\n## Certificates\\n\\n2025\\n\\n![IBM logo](https://peymankh.dev/assets/ibm-BCIOL5s0.png)\\n\\n### Deep Learning with Keras and Tensorflow\\n\\nIBM\\n\\n#### Skills Validated\\n\\nNeural Network\\n\\nDeep Learning\\n\\nPerformance Tuning\\n\\n[Verify Certificate](https://www.coursera.org/account/accomplishments/verify/GPBL8QRPVTNY)\\n\\n2025\\n\\n![Duke University logo](https://peymankh.dev/assets/duke-university-CSNfiEh2.png)\\n\\n### Cloud Virtualization, Containers and APIs\\n\\nDuke University\\n\\n#### Skills Validated\\n\\nContainerization\\n\\nKubernetes\\n\\nMicroservices\\n\\n[Verify Certificate](https://www.coursera.org/account/accomplishments/verify/CY512OI1WHIS)\\n\\n2024\\n\\n![IBM logo](https://peymankh.dev/assets/ibm-BCIOL5s0.png)\\n\\n### Data Analysis with Python\\n\\nIBM\\n\\n#### Skills Validated\\n\\nData Pipelines\\n\\nData Manipulation\\n\\nData Cleansing\\n\\n[Verify Certificate](https://www.coursera.org/account/accomplishments/verify/CFYFU8EH52DE)\\n\\n2024\\n\\n![Codecademy logo](https://peymankh.dev/assets/codecademy-CR_evvKR.png)\\n\\n### Data Structure & Algorithm\\n\\nCodecademy\\n\\n#### Skills Validated\\n\\nPattern Search\\n\\nSorting\\n\\nBrute Force\\n\\nGraph Search\\n\\n[Verify Certificate](https://www.codecademy.com/profiles/Peyman_kh/certificates/7a1021b263de1990c643feb15d9f1f7a)\\n\\n2024\\n\\n![DataCamp logo](https://peymankh.dev/assets/datacamp-C39Ae-zZ.png)\\n\\n### Prompt Engineering for Developers\\n\\nDataCamp\\n\\n#### Skills Validated\\n\\nLLM\\n\\nPrompt Strategies\\n\\n[Verify Certificate](https://www.datacamp.com/completed/statement-of-accomplishment/course/55bded8468b053b5c00926ce4544c1de0981f944)\\n\\n2023\\n\\n![DeepLearning.ai logo](https://peymankh.dev/assets/deeplearning-ai-Cluaecn1.png)\\n\\n### Mathematics for Machine Learning\\n\\nDeepLearning.ai\\n\\n#### Skills Validated\\n\\nCalculus\\n\\nLinear Algebra\\n\\nProbability\\n\\n[Verify Certificate](https://www.coursera.org/account/accomplishments/specialization/VKGPHD6TFKQ8)\\n\\n## Get In Touch\\n\\n### Send a Message\\n\\nFill out the form below and I'll get back to you within 24 hours.\\n\\nName\\n\\nEmail\\n\\nSubject\\n\\nMessage\\n\\nSend Message\\n\\n### Peyman KH\\n\\nAI Engineer & Developer passionate about LLM orchestration and intelligent agent systems.\\n\\n📍 Istanbul, Turkey\\n\\n### Connect\\n\\n[View my projects and contributions](https://github.com/PeymanKh) [Connect with me professionally](https://www.linkedin.com/in/peyman-khodabandehlouei) [peymankhodabandehlouei@gmail.com](mailto:peymankhodabandehlouei@gmail.com)\\n\\n### Status\\n\\nAvailable for opportunities\\n\\nOpen to new opportunities\\n\\nRemote & On-site positions\\n\\nUsually responds within 24 hours\\n\\n© 2025 peymankh.dev, All Rights Reserved.\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:34:07.416541Z",
     "start_time": "2025-09-26T16:34:07.414158Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f22f55a49abf6db2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "20ea027053afa19d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
